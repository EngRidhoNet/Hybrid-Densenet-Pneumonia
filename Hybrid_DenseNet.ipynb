{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+wxB/Vf8lts315HR9opmv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EngRidhoNet/Hybrid-Densenet-Pneumonia/blob/main/Hybrid_DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "h_MRH-WyJpgU",
        "outputId": "be48e864-319c-4adb-a2fd-e5e46e64e420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading dataset from Kaggle...\n",
            "üîÑ Preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 167MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training started...\n",
            "Epoch 1: Loss = 37.9822, Accuracy = 96.43%\n",
            "Epoch 2: Loss = 18.1776, Accuracy = 97.83%\n",
            "Epoch 3: Loss = 11.1206, Accuracy = 98.83%\n",
            "Epoch 4: Loss = 8.1092, Accuracy = 99.27%\n",
            "Epoch 5: Loss = 8.7736, Accuracy = 99.02%\n",
            "üß™ Evaluating on test set...\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.98      0.45      0.62       234\n",
            "   PNEUMONIA       0.75      0.99      0.86       390\n",
            "\n",
            "    accuracy                           0.79       624\n",
            "   macro avg       0.87      0.72      0.74       624\n",
            "weighted avg       0.84      0.79      0.77       624\n",
            "\n",
            "üìâ Confusion Matrix:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAHHCAYAAAAWM5p0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAUVBJREFUeJzt3Xl4TNf/B/D3TJbJngiyFbEmhNiVVGsNEaF2Yk1sLU20iC1+ShIlpNRWS7UqqFiL2lpiiVii1FcqtQRBQyWCyCJkst3fH77m25GEDHNzk8z71ec+T+ecc8/93HmET85yr0wQBAFEREREIpFLHQARERFVbEw2iIiISFRMNoiIiEhUTDaIiIhIVEw2iIiISFRMNoiIiEhUTDaIiIhIVEw2iIiISFRMNoiIiEhUTDaIRHTjxg107doVlpaWkMlk2LNnj1b7v3PnDmQyGcLDw7Xab3nWoUMHdOjQQeowiOhfmGxQhZeQkIBPP/0UtWvXhpGRESwsLNC2bVssW7YMz58/F/XaPj4+iIuLw7x587Bp0ya0bNlS1OuVJl9fX8hkMlhYWBT5Pd64cQMymQwymQyLFi3SuP/79+8jKCgIsbGxWoiWiKSkL3UARGI6cOAABgwYAIVCgREjRqBRo0bIycnBqVOnMHXqVFy+fBlr164V5drPnz9HTEwM/u///g/+/v6iXMPR0RHPnz+HgYGBKP2/ib6+Pp49e4Z9+/Zh4MCBanWbN2+GkZERsrOz36rv+/fvIzg4GDVr1kTTpk1LfN7hw4ff6npEJB4mG1Rh3b59G97e3nB0dMSxY8dgb2+vqvPz88PNmzdx4MAB0a7/8OFDAICVlZVo15DJZDAyMhKt/zdRKBRo27YttmzZUijZiIiIgJeXF37++edSieXZs2cwMTGBoaFhqVyPiEqO0yhUYYWFheHp06dYt26dWqLxUt26dfHFF1+oPufl5WHu3LmoU6cOFAoFatasiZkzZ0KpVKqdV7NmTfTo0QOnTp3C+++/DyMjI9SuXRsbN25UtQkKCoKjoyMAYOrUqZDJZKhZsyaAF9MPL///34KCgiCTydTKIiMj8eGHH8LKygpmZmZwdnbGzJkzVfXFrdk4duwYPvroI5iamsLKygq9evXC1atXi7zezZs34evrCysrK1haWmLkyJF49uxZ8V/sK4YMGYJff/0VaWlpqrLz58/jxo0bGDJkSKH2qampmDJlClxdXWFmZgYLCwt4enrizz//VLWJiopCq1atAAAjR45UTce8vM8OHTqgUaNGuHDhAtq1awcTExPV9/Lqmg0fHx8YGRkVun8PDw9UqlQJ9+/fL/G9EtHbYbJBFda+fftQu3ZtfPDBByVqP2bMGMyePRvNmzfHkiVL0L59e4SGhsLb27tQ25s3b6J///7o0qULFi9ejEqVKsHX1xeXL18GAPTt2xdLliwBAAwePBibNm3C0qVLNYr/8uXL6NGjB5RKJUJCQrB48WJ8/PHHOH369GvPO3LkCDw8PJCSkoKgoCBMnjwZZ86cQdu2bXHnzp1C7QcOHIjMzEyEhoZi4MCBCA8PR3BwcInj7Nu3L2QyGXbt2qUqi4iIQP369dG8efNC7W/duoU9e/agR48e+OabbzB16lTExcWhffv2qn/4GzRogJCQEADAJ598gk2bNmHTpk1o166dqp/Hjx/D09MTTZs2xdKlS9GxY8ci41u2bBmqVq0KHx8f5OfnAwC+++47HD58GCtWrICDg0OJ75WI3pJAVAGlp6cLAIRevXqVqH1sbKwAQBgzZoxa+ZQpUwQAwrFjx1Rljo6OAgAhOjpaVZaSkiIoFAohICBAVXb79m0BgPD111+r9enj4yM4OjoWimHOnDnCv38klyxZIgAQHj58WGzcL6+xfv16VVnTpk0FGxsb4fHjx6qyP//8U5DL5cKIESMKXW/UqFFqffbp00eoXLlysdf8932YmpoKgiAI/fv3Fzp37iwIgiDk5+cLdnZ2QnBwcJHfQXZ2tpCfn1/oPhQKhRASEqIqO3/+fKF7e6l9+/YCAGHNmjVF1rVv316t7NChQwIA4auvvhJu3bolmJmZCb17937jPRKRdnBkgyqkjIwMAIC5uXmJ2h88eBAAMHnyZLXygIAAACi0tsPFxQUfffSR6nPVqlXh7OyMW7duvXXMr3q51uOXX35BQUFBic5JSkpCbGwsfH19YW1trSpv3LgxunTporrPfxs3bpza548++giPHz9WfYclMWTIEERFRSE5ORnHjh1DcnJykVMowIt1HnL5i7968vPz8fjxY9UU0X/+858SX1OhUGDkyJElatu1a1d8+umnCAkJQd++fWFkZITvvvuuxNcionfDZIMqJAsLCwBAZmZmidr//fffkMvlqFu3rlq5nZ0drKys8Pfff6uV16hRo1AflSpVwpMnT94y4sIGDRqEtm3bYsyYMbC1tYW3tze2b9/+2sTjZZzOzs6F6ho0aIBHjx4hKytLrfzVe6lUqRIAaHQv3bt3h7m5ObZt24bNmzejVatWhb7LlwoKCrBkyRLUq1cPCoUCVapUQdWqVXHp0iWkp6eX+JrvvfeeRotBFy1aBGtra8TGxmL58uWwsbEp8blE9G6YbFCFZGFhAQcHB/z1118anffqAs3i6OnpFVkuCMJbX+PleoKXjI2NER0djSNHjmD48OG4dOkSBg0ahC5duhRq+y7e5V5eUigU6Nu3LzZs2IDdu3cXO6oBAPPnz8fkyZPRrl07/PTTTzh06BAiIyPRsGHDEo/gAC++H01cvHgRKSkpAIC4uDiNziWid8NkgyqsHj16ICEhATExMW9s6+joiIKCAty4cUOt/MGDB0hLS1PtLNGGSpUqqe3ceOnV0RMAkMvl6Ny5M7755htcuXIF8+bNw7Fjx3D8+PEi+34ZZ3x8fKG6a9euoUqVKjA1NX23GyjGkCFDcPHiRWRmZha5qPalnTt3omPHjli3bh28vb3RtWtXuLu7F/pOSpr4lURWVhZGjhwJFxcXfPLJJwgLC8P58+e11j8RvR6TDaqwpk2bBlNTU4wZMwYPHjwoVJ+QkIBly5YBeDENAKDQjpFvvvkGAODl5aW1uOrUqYP09HRcunRJVZaUlITdu3ertUtNTS107suHW726Hfcle3t7NG3aFBs2bFD7x/uvv/7C4cOHVfcpho4dO2Lu3Ln49ttvYWdnV2w7PT29QqMmO3bswD///KNW9jIpKiox09T06dORmJiIDRs24JtvvkHNmjXh4+NT7PdIRNrFh3pRhVWnTh1ERERg0KBBaNCggdoTRM+cOYMdO3bA19cXANCkSRP4+Phg7dq1SEtLQ/v27XHu3Dls2LABvXv3LnZb5dvw9vbG9OnT0adPH3z++ed49uwZVq9eDScnJ7UFkiEhIYiOjoaXlxccHR2RkpKCVatWoVq1avjwww+L7f/rr7+Gp6cn3NzcMHr0aDx//hwrVqyApaUlgoKCtHYfr5LL5Zg1a9Yb2/Xo0QMhISEYOXIkPvjgA8TFxWHz5s2oXbu2Wrs6derAysoKa9asgbm5OUxNTdG6dWvUqlVLo7iOHTuGVatWYc6cOaqtuOvXr0eHDh3w5ZdfIiwsTKP+iOgtSLwbhkh0169fF8aOHSvUrFlTMDQ0FMzNzYW2bdsKK1asELKzs1XtcnNzheDgYKFWrVqCgYGBUL16dSEwMFCtjSC82Prq5eVV6DqvbrksbuurIAjC4cOHhUaNGgmGhoaCs7Oz8NNPPxXa+nr06FGhV69egoODg2BoaCg4ODgIgwcPFq5fv17oGq9uDz1y5IjQtm1bwdjYWLCwsBB69uwpXLlyRa3Ny+u9urV2/fr1AgDh9u3bxX6ngqC+9bU4xW19DQgIEOzt7QVjY2Ohbdu2QkxMTJFbVn/55RfBxcVF0NfXV7vP9u3bCw0bNizymv/uJyMjQ3B0dBSaN28u5ObmqrWbNGmSIJfLhZiYmNfeAxG9O5kgaLAKjIiIiEhDXLNBREREomKyQURERKJiskFERESiYrJBREREomKyQURERKJiskFERESiYrJBREREoqqQTxCNvl74Mc9EBDx6zsdzE72qbxN70a9h3MxfK/08v/itVvopbRzZICIiIlFVyJENIiKiMkWm27/bM9kgIiISm0wmdQSSYrJBREQkNh0f2dDtuyciIiLRcWSDiIhIbJxGISIiIlFxGoWIiIhIPBzZICIiEhunUYiIiEhUnEYhIiIiEg9HNoiIiMTGaRQiIiISFadRiIiIiMTDkQ0iIiKxcRqFiIiIRKXj0yhMNoiIiMSm4yMbup1qERERkeg4skFERCQ2TqMQERGRqHQ82dDtuyciIiLRcWSDiIhIbHLdXiDKZIOIiEhsnEYhIiIiEg9HNoiIiMSm48/ZYLJBREQkNk6jEBEREYmHIxtERERi4zQKERERiUrHp1GYbBAREYlNx0c2dDvVIiIiItEx2SAiIhKbTK6dQwOrV69G48aNYWFhAQsLC7i5ueHXX39V1Xfo0AEymUztGDdunFofiYmJ8PLygomJCWxsbDB16lTk5eVpfPucRiEiIhKbBNMo1apVw4IFC1CvXj0IgoANGzagV69euHjxIho2bAgAGDt2LEJCQlTnmJiYqP4/Pz8fXl5esLOzw5kzZ5CUlIQRI0bAwMAA8+fP1ygWJhtEREQVUM+ePdU+z5s3D6tXr8bZs2dVyYaJiQns7OyKPP/w4cO4cuUKjhw5AltbWzRt2hRz587F9OnTERQUBENDwxLHwmkUIiIisUkwjfJv+fn52Lp1K7KysuDm5qYq37x5M6pUqYJGjRohMDAQz549U9XFxMTA1dUVtra2qjIPDw9kZGTg8uXLGl2fIxtERERi09I0ilKphFKpVCtTKBRQKBRFto+Li4Obmxuys7NhZmaG3bt3w8XFBQAwZMgQODo6wsHBAZcuXcL06dMRHx+PXbt2AQCSk5PVEg0Aqs/Jyckaxc1kg4iIqJwIDQ1FcHCwWtmcOXMQFBRUZHtnZ2fExsYiPT0dO3fuhI+PD06cOAEXFxd88sknqnaurq6wt7dH586dkZCQgDp16mg1biYbREREYtPSQ70CAwMxefJktbLiRjUAwNDQEHXr1gUAtGjRAufPn8eyZcvw3XffFWrbunVrAMDNmzdRp04d2NnZ4dy5c2ptHjx4AADFrvMoDtdsEBERiU1LazYUCoVqK+vL43XJxqsKCgoKTcO8FBsbCwCwt7cHALi5uSEuLg4pKSmqNpGRkbCwsFBNxZQURzaIiIgqoMDAQHh6eqJGjRrIzMxEREQEoqKicOjQISQkJCAiIgLdu3dH5cqVcenSJUyaNAnt2rVD48aNAQBdu3aFi4sLhg8fjrCwMCQnJ2PWrFnw8/PTKMEBmGwQERGJT4LnbKSkpGDEiBFISkqCpaUlGjdujEOHDqFLly64e/cujhw5gqVLlyIrKwvVq1dHv379MGvWLNX5enp62L9/P8aPHw83NzeYmprCx8dH7bkcJSUTBEHQ5s2VBdHXU6UOgahMevS86OFTIl3Wt4m96Ncw7lV4jcTbeP7Lp1rpp7RxZIOIiEhsfBEbERERkXg4skFERCQ2LW19La+YbBAREYmN0yhERERE4uHIBhERkchkOj6ywWSDiIhIZLqebHAahYiIiETFkQ0iIiKx6fbABpMNIiIisXEahYiIiEhEHNkgIiISma6PbDDZICIiEhmTDSIiIhKVricbXLNBREREouLIBhERkdh0e2CDyQYREZHYOI1CREREJCKObBAREYlM10c2mGwQERGJTNeTDU6jEBERkag4skFERCQyXR/ZYLJBREQkNt3ONcr2NMqlS5dgaGgodRhERET0Dsr0yIYgCMjPz5c6DCIionfCaRQiIiISFZMNIiIiEhWTDQllZGS8tj4zM7OUIiEiIiKxSJpsWFlZvTbbEwRB57NBIiKqAHT8nzJJk43jx49LeXkiIqJSoeu/OEuabLRv3/6NbVJTU0shEiIiIhJLmX3OxuHDhzFw4EC89957UodCRET0TmQymVaO8qpMJRt///035syZg5o1a2LAgAGQy+XYuHGj1GERERG9E11PNiTf+pqTk4Ndu3bhhx9+wOnTp+Hu7o579+7h4sWLcHV1lTo8IiIiekeSJhsTJkzAli1bUK9ePQwbNgzbtm1D5cqVYWBgAD09PSlDIyIi0pryPCqhDZImG6tXr8b06dMxY8YMmJubSxkKERGReHQ715B2zcamTZtw7tw52NvbY9CgQdi/fz/fhUJERFTBSJpsDB48GJGRkYiLi0P9+vXh5+cHOzs7FBQU4MqVK1KGRkREpDVSLBBdvXo1GjduDAsLC1hYWMDNzQ2//vqrqj47Oxt+fn6oXLkyzMzM0K9fPzx48ECtj8TERHh5ecHExAQ2NjaYOnUq8vLyNL7/MrEbpVatWggODsadO3fw008/oV+/fhg2bBiqVauGzz//XOrwiIiI3okUyUa1atWwYMECXLhwAX/88Qc6deqEXr164fLlywCASZMmYd++fdixYwdOnDiB+/fvo2/fvqrz8/Pz4eXlhZycHJw5cwYbNmxAeHg4Zs+erfn9C4IgaHxWKUhNTcXGjRuxfv16/PnnnxqdG32dDwIjKsqj50qpQyAqc/o2sRf9GtX9ftFKP3dX9nqn862trfH111+jf//+qFq1KiIiItC/f38AwLVr19CgQQPExMSgTZs2+PXXX9GjRw/cv38ftra2AIA1a9Zg+vTpePjwIQwNDUt83TIxslEUa2trTJw4UeNEg4iIqKJSKpXIyMhQO5TKN/8SkZ+fj61btyIrKwtubm64cOECcnNz4e7urmpTv3591KhRAzExMQCAmJgYuLq6qhINAPDw8EBGRoZqdKSkJN2NEhIS8sY2MpkMX375ZSlEQ0REJBIt7UYJDQ1FcHCwWtmcOXMQFBRUZPu4uDi4ubkhOzsbZmZm2L17N1xcXBAbGwtDQ0NYWVmptbe1tUVycjIAIDk5WS3ReFn/sk4TkiYbQUFBcHBwgI2NDYqbzWGyQURE5Z22nrMRGBiIyZMnq5UpFIpi2zs7OyM2Nhbp6enYuXMnfHx8cOLECa3EoglJkw1PT08cO3YMLVu2xKhRo9CjRw/I5WV2ZoeIiEhSCoXitcnFqwwNDVG3bl0AQIsWLXD+/HksW7YMgwYNQk5ODtLS0tRGNx48eAA7OzsAgJ2dHc6dO6fW38vdKi/blJSkycaBAwdw//59bNiwAVOnTsWnn36KESNGYNSoUXB2dpYyNHqN639dxKFdm/F3QjzSUx/hs5kL0Mztf2/wFQQBezd/j5OH9+JZVibqNmiMoZ9Ng61DdVWbGaP74HGK+jBc3xHj4TlgRKndB5E23b7yJ6L3bsU/t68j88ljDJsyFw3f/wgAkJ+Xh8Nb1yH+4lmkpiTByMQUdV1boNuQT2BhXUXVxz+3ruO3zd/hXsI1yOR6aNS6Hbx8PoPCyESq2yItKStPEC0oKIBSqUSLFi1gYGCAo0ePol+/fgCA+Ph4JCYmws3NDQDg5uaGefPmISUlBTY2NgCAyMhIWFhYwMXFRaPrSj6M4ODggMDAQMTHx2Pbtm1ISUlBq1at0LZtWzx//lzq8KgIyuxsVKtVD0PGBRRZ/9vPP+Ho/h0Y9tk0zFy0DoZGxlg6eyJyc9QXMfUaOhaLNu5XHZ16DiiN8IlEkaPMhn3NOug1emKhutycbNy/fR2d+o3AhIVrMSwgBA/v38XGsJmqNhmpj7BubgAq272Hz+avxsiZYUi5dwc7Vy4oxbsgsUix9TUwMBDR0dG4c+cO4uLiEBgYiKioKAwdOhSWlpYYPXo0Jk+ejOPHj+PChQsYOXIk3Nzc0KZNGwBA165d4eLiguHDh+PPP//EoUOHMGvWLPj5+Wk0ugKUgRex/VurVq1w584dXLlyBRcvXkRubi6MjY2lDote4drSDa4t3YqsEwQBR/dug9dAXzRt0w4AMGrSbAQM98LFs9F4v10XVVsjYxNYVqpcKjETic25WWs4N2tdZJ2RiRlGf7lYrezjUV9g1cxxSHv0AFZVbHHtPzHQ09fHx6MnqqaTe4+djGVTRuFR8j1Usasm+j1QxZKSkoIRI0YgKSkJlpaWaNy4MQ4dOoQuXV78PbxkyRLI5XL069cPSqUSHh4eWLVqlep8PT097N+/H+PHj4ebmxtMTU3h4+NTos0dryoTyUZMTAx+/PFHbN++HU5OThg5ciSGDBkCCwsLqUMjDT16cB/pTx6jQdNWqjITUzPUdnLBrWt/qSUbv+7chP3b1sO6qi1at+8K917e0NMrE38kiUSnfPYUMpkMRiZmAIC83Fzo6eurrVvT/+9zDP6+Fsdko5yTYhpl3bp1r603MjLCypUrsXLlymLbODo64uDBg+8ci6R/s4eFhSE8PByPHj3C0KFDcfLkSTRu3FjKkOgdpT95DACwsLJWKze3slbVAUCnngPhWMcZpmYWuHntEnZvWIO01McYNOaLUo2XSAq5OUr8unktGrftDCMTUwBAnUbNcGDjSkTv3YoPuvdDbnY2Dm1eCwDIfMIHFZZ7ZWPJhmQkTTZmzJiBGjVqYODAgZDJZAgPDy+y3TfffFNsH0qlstADTXJylDA01Gw+iUpX196DVf9frVZd6Osb4KeVC9HXZzwMDEr+VDqi8iY/Lw9blgQDENB7zCRVuW31WhjgF4gDG1biUMRayOR6+MCzL8wsK5WZxYVEb0vSZKNdu3aQyWSvfRLZm37IinrAia//NIycMF0rMZJmXq7ByEhLhdW/VtlnpqWiem2nYs+r5dQQ+fn5ePwgCXbVHEWPk0gK+Xl5iFgShCePHmDM7G9UoxovNf3QHU0/dEdmWioMjYwggwyn9u+Ata2DRBGTtuh6wihpshEVFfXOfRT1gJNziVnv3C+9nSq2DrCsVBnX/vwDNf6bXDx/loVb16+gffe+xZ539/YNyORymFtVKq1QiUrVy0TjcfI9jJmzFKbmlsW2Nf/vNOQfxw5C39AQdRu3KK0wSSRMNsq4P/74Ay1btiy2vqgHnBgaav76Wyq57OfPkJJ0T/X50YP7SLx1HaZmFqhsY4fOHw/CgW3hsHGojiq29vjlp+9hZV0Fzf67OyXhWhxux1+Gc+MWMDI2QcK1v7D9h2Vo08EDpmZcFEzlkzL7GR4n/6P6/CQlGffv3ICJmQXMrSpj8zdzcP/2dfhMD4VQkI/MtBdrmIzNLKCvbwAAOPPbLjg6NYKhkTFuXvoDv/60Bh5DPoGxqbkk90Tao+O5RtlINp4+fQo9PT21ba6xsbH48ssvcfDgQeTn50sYHb3q75vXsGimn+rz9nXLAQBunbpj1KQv0a3fMORkP8embxfgWdZT1HNpjC+Cl8Dgv+to9PUNce7kEezdsg55uTmoYusA916D0OVf6ziIypt/EuLxffD/1mAc2PhihX/z9h5wH+CLq3+cBgAsnzZG7byxc5agdsNmAIB7N6/hyPZw5GQ/R9X3aqD3JwFo3q5rKd0BkXgkfcX83bt3MXDgQJw7dw56enrw9/fHV199hXHjxmHbtm3o06cPJk2ahNati967Xhy+Yp6oaHzFPFFhpfGK+XpTf9NKPze+7qaVfkqbpCMbU6dORXZ2NpYtW4Zdu3Zh2bJlOHnyJFq3bo2EhARUq8Z95UREVP5xGkVC0dHR2LVrF9q0aYOBAwfCzs4OQ4cOxcSJE6UMi4iIiLRI0mTjwYMHqFWrFgDAxsYGJiYm8PT0lDIkIiIireNuFIn9+9G8crkchoZ8oBMREVUsOp5rSJtsCIIAJycnVcb39OlTNGvWTC0BAYDUVC74JCIiKq8kTTbWr18v5eWJiIhKhVyu20MbkiYbPj4+Ul6eiIioVHAapQx4/vw5IiMjcf36dQCAs7Mz3N3d1R7yRUREROWT5MnG3r17MWbMGDx69EitvEqVKli3bh169uwpUWRERETaoeu7UeRvbiKeM2fOoH///mjXrh1Onz6N1NRUpKam4tSpU/joo4/Qv39/nD17VsoQiYiI3plMpp2jvJL0ceXdu3dH9erV8d133xVZ/+mnn+Lu3bs4ePCgRv3yceVERePjyokKK43HlTeefUQr/VwKcddKP6VN0pGNs2fPwt/fv9h6Pz8/xMTElGJEREREpG2Srtl4/vw5LCyKf6W4paUlsrOzSzEiIiIi7eOaDQnVq1cPx44dK7b+6NGjqFevXilGREREpH26vmZD0mRj5MiRmDJlSpFrMg4cOIBp06bB19e39AMjIiIirZF0GuWLL77AmTNn0KNHDzg7O6NBgwYQBAFXr17FjRs30Lt3b74BloiIyj1Oo0h5cbkcO3bswJYtW+Dk5IRr164hPj4e9evXx+bNm/Hzzz8Xek8KERFReaPr0yiSP9QLAAYNGoRBgwZJHQYRERGJQNJkQy6Xv3FoSSaTIS8vr5QiIiIi0j5dn0aRNNnYvXt3sXUxMTFYvnw5CgoKSjEiIiIi7dPxXEPaZKNXr16FyuLj4zFjxgzs27cPQ4cORUhIiASRERERkbaUmdWX9+/fx9ixY+Hq6oq8vDzExsZiw4YNcHR0lDo0IiKidyKTybRylFeSJxvp6emYPn066tati8uXL+Po0aPYt28fGjVqJHVoREREWsHdKBIKCwvDwoULYWdnhy1bthQ5rUJERFTeledRCW2Q9K2vcrkcxsbGcHd3h56eXrHtdu3apVG/fOsrUdH41leiwkrjra+tQ09opZ/fA9trpZ/SJunIxogRI3Q+2yMioopP1/+pkzTZCA8Pl/LyREREpULXf7GWfIEoERERVWxl4nHlREREFZmOD2ww2SAiIhIbp1GIiIiowgkNDUWrVq1gbm4OGxsb9O7dG/Hx8WptOnToUOjBYePGjVNrk5iYCC8vL5iYmMDGxgZTp07V+J1lHNkgIiISmRQDGydOnICfnx9atWqFvLw8zJw5E127dsWVK1dgamqqajd27Fi1V4OYmJio/j8/Px9eXl6ws7PDmTNnkJSUhBEjRsDAwADz588vcSxMNoiIiEQmxTTKb7/9pvY5PDwcNjY2uHDhAtq1a6cqNzExgZ2dXZF9HD58GFeuXMGRI0dga2uLpk2bYu7cuZg+fTqCgoJgaGhYolg4jUJERFROKJVKZGRkqB1KZcke1peeng4AsLa2VivfvHkzqlSpgkaNGiEwMBDPnj1T1cXExMDV1RW2traqMg8PD2RkZODy5csljpvJBhERkci09SK20NBQWFpaqh2hoaFvvH5BQQEmTpyItm3bqr17bMiQIfjpp59w/PhxBAYGYtOmTRg2bJiqPjk5WS3RAKD6nJycXOL75zQKERGRyLQ1ixIYGIjJkyerlSkUijee5+fnh7/++gunTp1SK//kk09U/+/q6gp7e3t07twZCQkJqFOnjnaCBpMNIiIi0WlrzYZCoShRcvFv/v7+2L9/P6Kjo1GtWrXXtm3dujUA4ObNm6hTpw7s7Oxw7tw5tTYPHjwAgGLXeRSF0yhEREQVkCAI8Pf3x+7du3Hs2DHUqlXrjefExsYCAOztX7yczs3NDXFxcUhJSVG1iYyMhIWFBVxcXEocC0c2iIiIRCbF1lc/Pz9ERETgl19+gbm5uWqNhaWlJYyNjZGQkICIiAh0794dlStXxqVLlzBp0iS0a9cOjRs3BgB07doVLi4uGD58OMLCwpCcnIxZs2bBz89PoxEWJhtEREQik2Lr6+rVqwG8eHDXv61fvx6+vr4wNDTEkSNHsHTpUmRlZaF69ero168fZs2apWqrp6eH/fv3Y/z48XBzc4OpqSl8fHzUnstREkw2iIiIKiBBEF5bX716dZw4ceKN/Tg6OuLgwYPvFAuTDSIiIpHp+KtRmGwQERGJTa7j2QZ3oxAREZGoOLJBREQkMh0f2GCyQUREJDYpdqOUJUw2iIiIRCbX7VyDazaIiIhIXBzZICIiEhmnUYiIiEhUOp5rcBqFiIiIxMWRDSIiIpHJoNtDG0w2iIiIRMbdKEREREQi4sgGERGRyLgbhYiIiESl47kGp1GIiIhIXBzZICIiEpmuv2KeyQYREZHIdDzXYLJBREQkNl1fIMo1G0RERCQqjmwQERGJTMcHNphsEBERiU3XF4hyGoWIiIhExZENIiIiken2uAaTDSIiItFxNwoRERGRiDiyQUREJDJdf8V8iZKNvXv3lrjDjz/++K2DISIiqoh0fRqlRMlG7969S9SZTCZDfn7+u8RDREREFUyJko2CggKx4yAiIqqwdHxgg2s2iIiIxMZplLeQlZWFEydOIDExETk5OWp1n3/+uVYCIyIiqii4QFRDFy9eRPfu3fHs2TNkZWXB2toajx49gomJCWxsbJhsEBERkRqNn7MxadIk9OzZE0+ePIGxsTHOnj2Lv//+Gy1atMCiRYvEiJGIiKhck8lkWjnKK42TjdjYWAQEBEAul0NPTw9KpRLVq1dHWFgYZs6cKUaMRERE5ZpMS0d5pXGyYWBgALn8xWk2NjZITEwEAFhaWuLu3bvajY6IiIjKPY3XbDRr1gznz59HvXr10L59e8yePRuPHj3Cpk2b0KhRIzFiJCIiKtf4inkNzZ8/H/b29gCAefPmoVKlShg/fjwePnyItWvXaj1AIiKi8k4m086hidDQULRq1Qrm5uawsbFB7969ER8fr9YmOzsbfn5+qFy5MszMzNCvXz88ePBArU1iYiK8vLxUG0GmTp2KvLw8jWLReGSjZcuWqv+3sbHBb7/9pmkXREREJLITJ07Az88PrVq1Ql5eHmbOnImuXbviypUrMDU1BfBi08eBAwewY8cOWFpawt/fH3379sXp06cBAPn5+fDy8oKdnR3OnDmDpKQkjBgxAgYGBpg/f36JY5EJgiCIcpcSir6eKnUIRGXSo+dKqUMgKnP6NrEX/Rqf7LislX7WDmj41uc+fPgQNjY2OHHiBNq1a4f09HRUrVoVERER6N+/PwDg2rVraNCgAWJiYtCmTRv8+uuv6NGjB+7fvw9bW1sAwJo1azB9+nQ8fPgQhoaGJbq2xiMbtWrVeu32m1u3bmnaJRERUYWmrSUbSqUSSqX6Lw0KhQIKheKN56anpwMArK2tAQAXLlxAbm4u3N3dVW3q16+PGjVqqJKNmJgYuLq6qhINAPDw8MD48eNx+fJlNGvWrERxa5xsTJw4Ue1zbm4uLl68iN9++w1Tp07VtDsiIiIqodDQUAQHB6uVzZkzB0FBQa89r6CgABMnTkTbtm1VmzmSk5NhaGgIKysrtba2trZITk5Wtfl3ovGy/mVdSWmcbHzxxRdFlq9cuRJ//PGHpt0RERFVeNrajRIYGIjJkyerlZVkVMPPzw9//fUXTp06pZU4NKXxbpTieHp64ueff9ZWd0RERBWGtnajKBQKWFhYqB1vSjb8/f2xf/9+HD9+HNWqVVOV29nZIScnB2lpaWrtHzx4ADs7O1WbV3envPz8sk1JaC3Z2Llzp2oeiIiIiP5HiseVC4IAf39/7N69G8eOHUOtWrXU6lu0aAEDAwMcPXpUVRYfH4/ExES4ubkBANzc3BAXF4eUlBRVm8jISFhYWMDFxaXEsbzVQ73+fcOCICA5ORkPHz7EqlWrNO2OiIiIRODn54eIiAj88ssvMDc3V62xsLS0hLGxMSwtLTF69GhMnjwZ1tbWsLCwwIQJE+Dm5oY2bdoAALp27QoXFxcMHz4cYWFhSE5OxqxZs+Dn51ei6ZuXNN76GhQUpJZsyOVyVK1aFR06dED9+vU16Uo02Zo9a4RIZ1Rq5S91CERlzvOL34p+jQm7r2qlnxV9GpS4bXEjIevXr4evry+AFw/1CggIwJYtW6BUKuHh4YFVq1apTZH8/fffGD9+PKKiomBqagofHx8sWLAA+volH6+okM/ZYLJBVDQmG0SFlUay8fmea1rpZ3nvsvFLvaY0XrOhp6enNnfz0uPHj6Gnp6eVoIiIiKji0HjNRnEDIUqlssRPEiMiItIlct1+D1vJk43ly5cDeDEH9MMPP8DMzExVl5+fj+jo6DKzZoOIiKgsYbJRQkuWLAHwYmRjzZo1alMmhoaGqFmzJtasWaP9CImIiKhcK3Gycfv2bQBAx44dsWvXLlSqVEm0oIiIiCoSTZ+RUdFovGbj+PHjYsRBRERUYen6NIrGu1H69euHhQsXFioPCwvDgAEDtBIUERERVRwaJxvR0dHo3r17oXJPT09ER0drJSgiIqKKRFvvRimvNJ5Gefr0aZFbXA0MDJCRkaGVoIiIiCoSbb31tbzSeGTD1dUV27ZtK1S+detWjV7KQkREpCvkWjrKK41HNr788kv07dsXCQkJ6NSpEwDg6NGjiIiIwM6dO7UeIBEREZVvGicbPXv2xJ49ezB//nzs3LkTxsbGaNKkCY4dO8ZXzBMRERVBx2dRNE82AMDLywteXl4AgIyMDGzZsgVTpkzBhQsXkJ+fr9UAiYiIyjuu2XhL0dHR8PHxgYODAxYvXoxOnTrh7Nmz2oyNiIiIKgCNRjaSk5MRHh6OdevWISMjAwMHDoRSqcSePXu4OJSIiKgYOj6wUfKRjZ49e8LZ2RmXLl3C0qVLcf/+faxYsULM2IiIiCoEuUw7R3lV4pGNX3/9FZ9//jnGjx+PevXqiRkTERERVSAlHtk4deoUMjMz0aJFC7Ru3RrffvstHj16JGZsREREFYJcJtPKUV6VONlo06YNvv/+eyQlJeHTTz/F1q1b4eDggIKCAkRGRiIzM1PMOImIiMotXX9cuca7UUxNTTFq1CicOnUKcXFxCAgIwIIFC2BjY4OPP/5YjBiJiIioHHunp586OzsjLCwM9+7dw5YtW7QVExERUYXCBaJaoKenh969e6N3797a6I6IiKhCkaEcZwpaoJVkg4iIiIpXnkcltKE8v0SOiIiIygGObBAREYlM10c2mGwQERGJTFae961qAadRiIiISFQc2SAiIhIZp1GIiIhIVDo+i8JpFCIiIhIXRzaIiIhEVp5foqYNTDaIiIhEputrNjiNQkRERKLiyAYREZHIdHwWhckGERGR2OR8ERsRERGJSddHNrhmg4iIiETFZIOIiEhkcpl2Dk1FR0ejZ8+ecHBwgEwmw549e9TqfX19IZPJ1I5u3bqptUlNTcXQoUNhYWEBKysrjB49Gk+fPtXs/jUPnYiIiDQhl8m0cmgqKysLTZo0wcqVK4tt061bNyQlJamOLVu2qNUPHToUly9fRmRkJPbv34/o6Gh88sknGsXBNRtEREQVlKenJzw9PV/bRqFQwM7Orsi6q1ev4rfffsP58+fRsmVLAMCKFSvQvXt3LFq0CA4ODiWKgyMbREREIpPJtHMolUpkZGSoHUql8p1ii4qKgo2NDZydnTF+/Hg8fvxYVRcTEwMrKytVogEA7u7ukMvl+P3330t8DSYbREREItPWNEpoaCgsLS3VjtDQ0LeOq1u3bti4cSOOHj2KhQsX4sSJE/D09ER+fj4AIDk5GTY2Nmrn6Ovrw9raGsnJySW+DqdRiIiIyonAwEBMnjxZrUyhULx1f97e3qr/d3V1RePGjVGnTh1ERUWhc+fOb93vq5hsEBERiUxbz9lQKBTvlFy8Se3atVGlShXcvHkTnTt3hp2dHVJSUtTa5OXlITU1tdh1HkXhNAoREZHI5Fo6xHbv3j08fvwY9vb2AAA3NzekpaXhwoULqjbHjh1DQUEBWrduXeJ+ObJBRERUQT19+hQ3b95Ufb59+zZiY2NhbW0Na2trBAcHo1+/frCzs0NCQgKmTZuGunXrwsPDAwDQoEEDdOvWDWPHjsWaNWuQm5sLf39/eHt7l3gnCsCRDSIiItG9+uCstz009ccff6BZs2Zo1qwZAGDy5Mlo1qwZZs+eDT09PVy6dAkff/wxnJycMHr0aLRo0QInT55Um6rZvHkz6tevj86dO6N79+748MMPsXbtWs3uXxAEQePoy7jsPKkjICqbKrXylzoEojLn+cVvRb/Gxj/uaqWfES2ra6Wf0sZpFCIiIpG9zdM/KxJOoxAREZGoOLJBREQkMt0e1ygHIxupqalSh0BERPROtPW48vKqzCYbhw8fxsCBA/Hee+9JHQoRERG9gzKVbPz999+YM2cOatasiQEDBkAul2Pjxo1Sh0VERPROpNr6WlZIvmYjJycHu3btwg8//IDTp0/D3d0d9+7dw8WLF+Hq6ip1eERERO+sTP1mLwFJ73/ChAlwcHDAsmXL0KdPH9y7dw/79u2DTCaDnp6elKERERGRlkg6srF69WpMnz4dM2bMgLm5uZShEBERiaY8T4Fog6QjG5s2bcK5c+dgb2+PQYMGYf/+/cjPz5cyJCIiIq2TaekoryRNNgYPHozIyEjExcWhfv368PPzg52dHQoKCnDlyhUpQyMiIiItKRNrVmrVqoXg4GDcuXMHP/30E/r164dhw4ahWrVq+Pzzz6UOj4iI6J1wN0oZIpPJ4OHhAQ8PD6SmpmLjxo1Yv3691GERERG9kzLxm72Eyuz9W1tbY+LEifjzzz+lDoWIiOidcGRDQpMnT35jG5lMhsWLF5dCNERERCQGSZONixcvvrFNec7kiIiIgPK9k0QbJE02jh8/LuXliYiISoWu/95cZtdsEBERUcUg6chGSEhIidrNnj1b5EiIiIjEI9fxiRRJk43du3cXWyeTyRAfH4/s7GwmG0REVK7p+jRKmVwgGhsbixkzZuCvv/7C2LFjSzkqIiIi0qYytWbj9u3bGDZsGFq1agVLS0tcvnwZa9askTosIiKidyLT0n/lVZlINh49eoQJEyagfv36SEpKwpkzZ7Bt2zbUq1dP6tCIiIjemUymnaO8knQaJSsrC4sWLcI333yDunXrYt++fejatauUIREREZGWSZps1KlTB5mZmZgwYQIGDx4MmUyGS5cuFWrXuHFjCaIjIiLSDl3fjSITBEGQ6uJy+f9mcWQyGf4dysvPMpkM+fn5GvWbnae1EIkqlEqt/KUOgajMeX7xW9GvcejKQ6304+FSVSv9lDZJRzZu374t5eWJiIhKRXleb6ENkiYbjo6OUl6eiIiISoGkycZL58+fx5YtW3D9+nUAgJOTE4YMGYKWLVtKHBkREdG7K8/bVrVB8q2v06ZNQ+vWrfHDDz/g3r17uHfvHr7//nu0bt0a06dPlzo8IiKidyaXaecoryRNNjZs2IAVK1Zg+fLlePz4MWJjYxEbG4vU1FQsWbIEy5cvx8aNG6UMkYiIiN6RpNMoK1euxPz58+Hvr75C3sDAAJ9//jny8vLw7bffYsSIERJFSERE9O44jSKhy5cvo1evXsXW9+7dG5cvXy7FiIiIiLRP158gKmmyoaenh5ycnGLrc3NzoaenV4oRERERkbZJmmw0b94cmzdvLrZ+06ZNaN68eSlGREREpH26/iI2SddsTJkyBb1794ZSqURAQABsbW0BAMnJyVi8eDGWLl2K3bt3SxkiERHROyvPO0m0QdJko0ePHliyZAmmTJmCxYsXw9LSEgCQnp4OfX19LFq0CD169JAyRCIiInpHkj9nY8KECUhISMCiRYvg7e0Nb29vLF68GDdv3sQXX3whdXj0FtZ9/x2GDOwHt1bN0OEjN0yc8Bnu3L4ldVhEoho74EOc2xaIBye/xoOTXyNqQwC6tnVR1dtWNse6uSNwO3I+Hp1ZjDMR09G7c1O1PurWsMH2JZ/g7rEFeHDyaxz9cRLataxXyndCYpBqGiU6Oho9e/aEg4MDZDIZ9uzZo1YvCAJmz54Ne3t7GBsbw93dHTdu3FBrk5qaiqFDh8LCwgJWVlYYPXo0nj59qlEcZeIJotWqVcOkSZOkDoO05I/z5zBo8FA0dHVFfl4+Viz7BuPGjsauvQdgYmIidXhEovjnQRq+XPELbiY+hAwyDOvZGjuWfII23gtw9VYyfpg7Albmxhgw8Ts8SnuKQZ4t8dPCUWg7NAx/xt8DAOxaPg43E1Pg+elyPFfmwn9IR+xaPg4NewbhweNMie+Q3oVUO0mysrLQpEkTjBo1Cn379i1UHxYWhuXLl2PDhg2oVasWvvzyS3h4eODKlSswMjICAAwdOhRJSUmIjIxEbm4uRo4ciU8++QQREREljkPSt75GR0eXqF27du006pdvfS1bUlNT0fEjN/y44Se0aNlK6nB0Gt/6Wrr+iVqImUv3YMOeGDw8vRifz9+KLQfOq+rvHV+IWcv3IHx3DCpbmeLe8YVwH7UEpy8mAADMTBR4eHoxuo9bgeO/x0t1GxVeabz19fSNJ1rpp229Sm99rkwmw+7du9G7d28AL0Y1HBwcEBAQgClTpgB4sYzB1tYW4eHh8Pb2xtWrV+Hi4oLz58+rXiHy22+/oXv37rh37x4cHBxKdG1JRzY6dOgA2X/TveJynrd5xTyVLU8zX/xGZvHfNTlEFZ1cLkO/Ls1hamyI3y+9eLv12T9voX/XFvjt5GWkZT5H/67NYaTQR/QfL4asH6dlIf52Mob0eB8Xr96FMjcPY/p9iAePM3DxSqKUt0NliFKphFKpVCtTKBRQKBQa93X79m0kJyfD3d1dVWZpaYnWrVsjJiYG3t7eiImJgZWVldq7ytzd3SGXy/H777+jT58+JbqWpMlGpUqVYG5uDl9fXwwfPhxVqlTRuI+ivnhB7+2+eNK+goIChC2cj6bNmqNePSepwyESVcO6DojaEAAjQ308fa7EoIDvce1WMgBg2LQfsWnhKNw/EYbc3Hw8y87BoMnf49bdR6rzvcZ9i21LPsHD04tQUCDg4ZOn6OW3CmmZz6W6JdISuZbmUUJDQxEcHKxWNmfOHAQFBWncV3Lyiz+bL3eCvmRra6uqS05Oho2NjVq9vr4+rK2tVW1KQtIFoklJSVi4cCFiYmLg6uqK0aNH48yZM7CwsIClpaXqeJ3Q0FC1tpaWlvh6YWgp3QG9yfyvgpFw4wbCFi2ROhQi0V2/8wCtvUPRbsQifL/jFL4PGY76te0AAHP8esDK3Bieny5H22FhWP7TMfwUNgoN6/5vGHpJ4EA8TM2E+6il+Gj419h7/E/8vOxT2FWxkOqWSEtkWjoCAwORnp6udgQGBpb27WhM0mTD0NAQgwYNwqFDh3Dt2jU0btwY/v7+qF69Ov7v//4PeXlvXnxR1Bc/dXrZ/+J1wfyvQhB9Igrfr98AWzs7qcMhEl1uXj5u3X2Ei1fvYvaKvYi7/g/8BndArWpVMN67PT4N+glR564j7vo/mL/2V/znSiI+HfRiTVqH953Q/aNGGDFjPWL+vIXYa/cwMXQ7nitzMaxna4nvjMoKhUIBCwsLteNtR/Lt/vv38oMHD9TKHzx4oKqzs7NDSkqKWn1eXh5SU1NVbUpC8q2vL9WoUQOzZ8/GkSNH4OTkhAULFiAjI+ON52nziyftEAQB878KwbGjkfj+xw2oVq261CERSUIuk0FhqA8TI0MAQMEra9Py8wXV8LqqTUGBWpuCAkG1to3KMW0NbWhRrVq1YGdnh6NHj6rKMjIy8Pvvv8PNzQ0A4ObmhrS0NFy4cEHV5tixYygoKEDr1iVPgstEsqFUKhEREQF3d3c0atQIVapUwYEDB2BtbS11aPQW5s8NxsH9e7EgbDFMTUzx6OFDPHr4ENnZ2VKHRiSakAkfo23zOqhhb42GdR0QMuFjtGtZD1sP/oH4O8m4mZiCb2cNRsuGjqhVrQq+GN4Jnds4Y1/UnwCA3y/dxpOMZ/hh7gi4Or2HujVsMH9ib9R8rzJ+O8UXUpZ3Uj1n4+nTp4iNjUVsbCyAF4tCY2NjkZiYCJlMhokTJ+Krr77C3r17ERcXhxEjRsDBwUG1Y6VBgwbo1q0bxo4di3PnzuH06dPw9/eHt7d3iXeiABJvfT137hzWr1+PrVu3ombNmhg5ciSGDRv2zkkGt75Kq0lD5yLLQ74KRa8+hfd5U+nh1lfxrJ4zBB3fd4ZdFQukP83GXzf+weL1R3Ds92sAgDo1quKrz3vBrWltmJkokHD3IZZuPKq2Fba5Sw0E+fVEc5caMNCX4+qtZMxf+ysOn74i1W3phNLY+vp7QrpW+mldR7NdfVFRUejYsWOhch8fH4SHh0MQBMyZMwdr165FWloaPvzwQ6xatQpOTv9b0J+amgp/f3/s27cPcrkc/fr1w/Lly2FmZlbiOCRNNuRyOWrUqAEfHx+0aNGi2HYff/yxRv0y2SAqGpMNosJKI9k4d0s7ycb7tcvnIwQkf4JoYmIi5s6dW2w9n7NBRETlna6vupE02Xh1IRQRERFVPJKPbBAREVV4Oj60IWmysXz58iLLLS0t4eTkpNp6Q0REVJ69zU6SikTSZGPJkqKfKpmWlob09HR88MEH2Lt3L7fAEhFRuabrj0qR9Dkbt2/fLvJ48uQJbt68iYKCAsyaNUvKEImIiOgdlYmHehWldu3aWLBgAQ4fPix1KERERO+kDD5AtFSV6QWiNWrU0OitckRERGVSec4UtKDMjmwAQFxcHBwdHaUOg4iIiN6BpCMbxb1oLT09HRcuXEBAQAB8fHxKOSoiIiLt4m4UCVlZWRX7NkOZTIYxY8ZgxowZpRwVERGRdun6bhRJk43jx48XWW5hYYF69epp9JIXIiIiKpskTTbat28v5eWJiIhKhY4PbEi7QDQsLAzPnz9XfT59+jSUSqXqc2ZmJj777DMpQiMiItIeHd/7KmmyERgYiMzMTNVnT09P/PPPP6rPz549w3fffSdFaERERKQlkk6jCILw2s9EREQVAXejEBERkai4G4WIiIhEpeO5hvTJxg8//KDa4pqXl4fw8HBUqVIFANTWcxAREVH5JBMkXChRs2bNYh/q9W+3b9/WqN/svLeNiKhiq9TKX+oQiMqc5xe/Ff0af/3zVCv9NHqvfD5/StKRjTt37kh5eSIiolKh6wtEy/SL2IiIiKj8k3RkY+PGjSVqN2LECJEjISIiEo+u70aRdM1GpUqViq2TyWTIyspCXl4e8vPzNeqXazaIisY1G0SFlcaajav3s7TSTwMHU630U9oknUZ58uRJkceVK1cwcOBACIKALl26SBkiERERvaMytWYjMzMTs2bNgpOTE2JjY3Ho0CH89ttvUodFRET0bnT83SiSP2cDAHJzc7FixQrMnz8flStXxvr169G/f3+pwyIiItIKXd+NIvm7UTZu3IjZs2cjLy8P8+fPx+jRo6GnpydlWERERKRFkiYbjRs3xq1btzBhwgRMnDgRJiYmyMoqvIjGwsJCguiIiIi0g7tRJNyNIpf/b8lIUU8SFQQBMpmMu1GItIS7UYgKK43dKNeTn2mlHyc7E630U9okHdk4fvy4lJcnIiIqHTo+siFpsvHhhx9i0aJF2Lt3L3JyctC5c2fMmTMHxsbGUoZFREREWiTp1tf58+dj5syZMDMzw3vvvYdly5bBz89PypCIiIi0Tqal/8orSZONjRs3YtWqVTh06BD27NmDffv2YfPmzSgoKJAyLCIiIq2SybRzlFeSJhuJiYno3r276rO7uztkMhnu378vYVRERESkTZKu2cjLy4ORkZFamYGBAXJzcyWKiIiISPvK8aCEVkj+UC9fX18oFApVWXZ2NsaNGwdT0/+9bGbXrl1ShEdERKQdEmQbQUFBCA4OVitzdnbGtWvXALz49zYgIABbt26FUqmEh4cHVq1aBVtbW63HImmy4ePjU6hs2LBhEkRCRERU8TRs2BBHjhxRfdbX/98/+5MmTcKBAwewY8cOWFpawt/fH3379sXp06e1Hoekycb69eulvDwREVGpkGonib6+Puzs7AqVp6enY926dYiIiECnTp0AvPg3uUGDBjh79izatGmj1TjK1FtfiYiIKiKpdqPcuHEDDg4OqF27NoYOHYrExEQAwIULF5Cbmwt3d3dV2/r166NGjRqIiYnR1m2rlIm3vhIREdGbKZVKKJVKtTKFQqG29vGl1q1bIzw8HM7OzkhKSkJwcDA++ugj/PXXX0hOToahoSGsrKzUzrG1tUVycrLW4+bIBhERkchkWjpCQ0NhaWmpdoSGhhZ5TU9PTwwYMACNGzeGh4cHDh48iLS0NGzfvl3Uey0KRzaIiIjEpqUlG4GBgZg8ebJaWVGjGkWxsrKCk5MTbt68iS5duiAnJwdpaWlqoxsPHjwoco3Hu+LIBhERkci09bhyhUIBCwsLtaOkycbTp0+RkJAAe3t7tGjRAgYGBjh69KiqPj4+HomJiXBzc9P6/XNkg4iIqAKaMmUKevbsCUdHR9y/fx9z5syBnp4eBg8eDEtLS4wePRqTJ0+GtbU1LCwsMGHCBLi5uWl9JwrAZIOIiEh0UrzX5N69exg8eDAeP36MqlWr4sMPP8TZs2dRtWpVAMCSJUsgl8vRr18/tYd6iUEmCIIgSs8Sys6TOgKisqlSK3+pQyAqc55f/Fb0a9xNVb65UQlUty7ZlElZwzUbREREJCpOoxAREYmsPL8eXhuYbBAREYlOt7MNTqMQERGRqDiyQUREJDJOoxAREZGodDzX4DQKERERiYsjG0RERCLjNAoRERGJSqbjEylMNoiIiMSm27kG12wQERGRuDiyQUREJDIdH9hgskFERCQ2XV8gymkUIiIiEhVHNoiIiETG3ShEREQkLt3ONTiNQkREROLiyAYREZHIdHxgg8kGERGR2LgbhYiIiEhEHNkgIiISGXejEBERkag4jUJEREQkIiYbREREJCpOoxAREYlM16dRmGwQERGJTNcXiHIahYiIiETFkQ0iIiKRcRqFiIiIRKXjuQanUYiIiEhcHNkgIiISm44PbTDZICIiEhl3oxARERGJiCMbREREIuNuFCIiIhKVjucaTDaIiIhEp+PZBtdsEBERkag4skFERCQyXd+NwmSDiIhIZLq+QJTTKERERCQqmSAIgtRBUMWkVCoRGhqKwMBAKBQKqcMhKjP4s0G6hskGiSYjIwOWlpZIT0+HhYWF1OEQlRn82SBdw2kUIiIiEhWTDSIiIhIVkw0iIiISFZMNEo1CocCcOXO4AI7oFfzZIF3DBaJEREQkKo5sEBERkaiYbBAREZGomGwQERGRqJhsEBERkaiYbOgYX19fyGQyLFiwQK18z549kP3rTUH5+flYsmQJXF1dYWRkhEqVKsHT0xOnT59WOy88PBwymQwymQxyuRz29vYYNGgQEhMT1dp16NChyOsCgJeXF2QyGYKCggrVbdmyBXp6evDz8ytUFxUVBZlMhrS0NA2+AdIFL/+cy2QyGBoaom7duggJCUFeXp7qz03Dhg2Rn5+vdp6VlRXCw8NVn2vWrKnq59/Hyz/Hr/szWLNmTSxdulT1+eW5Z8+eVWunVCpRuXJlyGQyREVFqdXt378f7du3h7m5OUxMTNCqVSu1+ADgzp07kMlksLGxQWZmplpd06ZN1X6uOnTogIkTJxaK9XU/Z0TawGRDBxkZGWHhwoV48uRJkfWCIMDb2xshISH44osvcPXqVURFRaF69ero0KED9uzZo9bewsICSUlJ+Oeff/Dzzz8jPj4eAwYMKNRv9erVC/1F+c8//+Do0aOwt7cvMpZ169Zh2rRp2LJlC7Kzs9/qfkk3devWDUlJSbhx4wYCAgIQFBSEr7/+WlV/69YtbNy48Y39hISEICkpSe2YMGHCW8VUvXp1rF+/Xq1s9+7dMDMzK9R2xYoV6NWrF9q2bYvff/8dly5dgre3N8aNG4cpU6YUap+ZmYlFixa9VVz8OSOxMdnQQe7u7rCzs0NoaGiR9du3b8fOnTuxceNGjBkzBrVq1UKTJk2wdu1afPzxxxgzZgyysrJU7WUyGezs7GBvb48PPvgAo0ePxrlz55CRkaHWb48ePfDo0SO10ZENGzaga9eusLGxKRTH7du3cebMGcyYMQNOTk7YtWuXlr4B0gUKhQJ2dnZwdHTE+PHj4e7ujr1796rqJ0yYgDlz5kCpVL62H3Nzc9jZ2akdpqambxWTj48Ptm7diufPn6vKfvzxR/j4+Ki1u3v3LgICAjBx4kTMnz8fLi4uqFu3LgICAvD1119j8eLF+P3339XOmTBhAr755hukpKRoFBN/zqg0MNnQQXp6epg/fz5WrFiBe/fuFaqPiIiAk5MTevbsWaguICAAjx8/RmRkZJF9p6SkYPfu3dDT04Oenp5anaGhIYYOHar2m114eDhGjRpVZF/r16+Hl5cXLC0tMWzYMKxbt06T2yRSY2xsjJycHNXniRMnIi8vDytWrCi1GFq0aIGaNWvi559/BgAkJiYiOjoaw4cPV2u3c+dO5ObmFjmC8emnn8LMzAxbtmxRKx88eLBqukgT/Dmj0sBkQ0f16dMHTZs2xZw5cwrVXb9+HQ0aNCjyvJfl169fV5Wlp6fDzMwMpqamsLW1xfHjx+Hn51fkb3+jRo3C9u3bkZWVhejoaKSnp6NHjx6F2hUUFCA8PBzDhg0DAHh7e+PUqVO4ffv2W90v6S5BEHDkyBEcOnQInTp1UpWbmJhgzpw5CA0NRXp6erHnT58+HWZmZmrHyZMn3zqeUaNG4ccffwTwItnu3r07qlatqtbm+vXrsLS0LHJ60dDQELVr11b7GQSgWkuydu1aJCQklCgW/pxRaWGyocMWLlyIDRs24OrVq4XqNHmwrLm5OWJjY/HHH39g8eLFaN68OebNm1dk2yZNmqBevXrYuXMnfvzxRwwfPhz6+vqF2kVGRiIrKwvdu3cHAFSpUgVdunRR/SVN9Cb79++HmZkZjIyM4OnpiUGDBhVahDx69GhUrlwZCxcuLLafqVOnIjY2Vu1o2bLlW8c1bNgwxMTE4NatW68d2XsbHh4e+PDDD/Hll1+WqD1/zqi0FP5bnnRGu3bt4OHhgcDAQPj6+qrKnZycikxAAKjKnZycVGVyuRx169YF8GLkIyEhAePHj8emTZuK7GPUqFFYuXIlrly5gnPnzhXZZt26dUhNTYWxsbGqrKCgAJcuXUJwcDDkcubJ9HodO3bE6tWrYWhoCAcHhyKTWn19fcybNw++vr7w9/cvsp8qVaqo/ny/ysLCAsCL0T0rKyu1urS0NFhaWhY6p3LlyujRowdGjx6N7OxseHp6FtpF4uTkhPT0dNy/fx8ODg5qdTk5OUhISEDHjh2LjGnBggVwc3PD1KlTi6z/N/6cUWnhnyQdt2DBAuzbtw8xMTGqMm9vb9y4cQP79u0r1H7x4sWoXLkyunTpUmyfM2bMwLZt2/Cf//ynyPohQ4YgLi4OjRo1gouLS6H6x48f45dffsHWrVvVfpu8ePEinjx5gsOHD7/FnZKuMTU1Rd26dVGjRo0iE42XBgwYgIYNGyI4OFjja9SrVw9yuRwXLlxQK7916xbS09PVkvJ/GzVqFKKiojBixIhCa5sAoF+/fjAwMMDixYsL1a1ZswZZWVkYPHhwkX2///776Nu3L2bMmPHa2PlzRqWJIxs6ztXVFUOHDsXy5ctVZd7e3tixYwd8fHzw9ddfo3PnzsjIyMDKlSuxd+9e7Nix47Wr8atXr44+ffpg9uzZ2L9/f6H6SpUqISkpCQYGBkWev2nTJlSuXBkDBw5Ue/YHAHTv3h3r1q1Dt27dVGVxcXEwNzdXfZbJZGjSpEmJvwOiBQsWwMPDo8i6zMxMJCcnq5WZmJjAwsIC5ubmGDNmDAICAqCvrw9XV1fcvXsX06dPR5s2bfDBBx8U2We3bt3w8OFD1cjIq2rUqIGwsDAEBATAyMgIw4cPh4GBAX755RfMnDkTAQEBaN26dbH3M2/ePDRs2PC1SZamP2dE74IjG4SQkBAUFBSoPstkMmzfvh0zZ87EkiVL4OzsjI8++gh///03oqKi0Lt37zf2OWnSJBw4cKDYaRIrK6tiE5Yff/wRffr0KfQXIPDiN769e/fi0aNHqrJ27dqhWbNmqqNFixZvjI/o3zp16oROnTohLy+vUN3s2bNhb2+vdkybNk1Vv2zZMvj4+GD69Olo2LAhfH190bhxY+zbt6/IP8PAi5+xKlWqwNDQsNiYJk6ciN27d+PkyZNo2bIlGjVqhIiICKxevfqNz9NwcnLCqFGjXvvMDE1/zojeBV8xT0RERKLiyAYRERGJiskGERERiYrJBhEREYmKyQYRERGJiskGERERiYrJBhEREYmKyQYRERGJiskGUQXk6+ur9vC1Dh06YOLEiaUeR1RUFGQyGdLS0kr92kRUdjDZICpFvr6+kMlkkMlkMDQ0RN26dRESElLkkyu1adeuXZg7d26J2jJBICJt47tRiEpZt27dsH79eiiVShw8eBB+fn4wMDBAYGCgWrucnJzXPs5aE9bW1lrph4jobXBkg6iUKRQK2NnZwdHREePHj4e7uzv27t2rmvqYN28eHBwc4OzsDAC4e/cuBg4cCCsrK1hbW6NXr164c+eOqr/8/HxMnjwZVlZWqFy5MqZNm4ZX30Lw6jSKUqnE9OnTUb16dSgUCtStWxfr1q3DnTt3VK8ur1SpEmQyGXx9fQG8ePV4aGgoatWqBWNjYzRp0gQ7d+5Uu87Bgwfh5OQEY2NjdOzYUS1OItJdTDaIJGZsbIycnBwAwNGjRxEfH4/IyEjs378fubm58PDwgLm5OU6ePInTp0/DzMwM3bp1U52zePFihIeH48cff8SpU6eQmpqK3bt3v/aaI0aMwJYtW7B8+XJcvXoV3333HczMzFC9enX8/PPPAID4+HgkJSVh2bJlAIDQ0FBs3LgRa9asweXLlzFp0iQMGzYMJ06cAPAiKerbty969uyJ2NhYjBkz5o2vOSciHSEQUanx8fERevXqJQiCIBQUFAiRkZGCQqEQpkyZIvj4+Ai2traCUqlUtd+0aZPg7OwsFBQUqMqUSqVgbGwsHDp0SBAEQbC3txfCwsJU9bm5uUK1atVU1xEEQWjfvr3wxRdfCIIgCPHx8QIAITIyssgYjx8/LgAQnjx5oirLzs4WTExMhDNnzqi1HT16tDB48GBBEAQhMDBQcHFxUaufPn16ob6ISPdwzQZRKdu/fz/MzMyQm5uLgoICDBkyBEFBQfDz84Orq6vaOo0///wTN2/ehLm5uVof2dnZSEhIQHp6OpKSktC6dWtVnb6+Plq2bFloKuWl2NhY6OnpoX379iWO+ebNm3j27Bm6dOmiVp6Tk4NmzZoBAK5evaoWBwC4ubmV+BpEVHEx2SAqZR07dsTq1athaGgIBwcH6Ov/78fQ1NRUre3Tp0/RokULbN68uVA/VatWfavrGxsba3zO06dPAQAHDhzAe++9p1anUCjeKg4i0h1MNohKmampKerWrVuits2bN8e2bdtgY2MDCwuLItvY29vj999/R7t27QAAeXl5uHDhApo3b15ke1dXVxQUFODEiRNwd3cvVP9yZCU/P19V5uLiAoVCgcTExGJHRBo0aIC9e/eqlZ09e/bNN0lEFR4XiBKVYUOHDkWVKlXQq1cvnDx5Erdv30ZUVBQ+//xz3Lt3DwDwxRdfYMGCBdizZw+uXbuGzz777LXPyKhZsyZ8fHwwatQo7NmzR9Xn9u3bAQCOjo6QyWTYv38/Hj58iKdPn8Lc3BxTpkzBpEmTsGHDBiQkJOA///kPVqxYgQ0bNgAAxo0bhxs3bmDq1KmIj49HREQEwsPDxf6KiKgcYLJBVIaZmJggOjoaNWrUQN++fdGgQQOMHj0a2dnZqpGOgIAADB8+HD4+PnBzc4O5uTn69Onz2n5Xr16N/v3747PPPkP9+vUxduxYZGVlAQDee+89BAcHY8aMGbC1tYW/vz8AYO7cufjyyy8RGhqKBg0aoFu3bjhw4ABq1aoFAKhRowZ+/vln7NmzB02aNMGaNWswf/58Eb8dIiovZEJxq8iIiIiItIAjG0RERCQqJhtEREQkKiYbREREJComG0RERCQqJhtEREQkKiYbREREJComG0RERCQqJhtEREQkKiYbREREJComG0RERCQqJhtEREQkKiYbREREJKr/B8RASR3LIO2GAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# ========== 1. Download Dataset ==========\n",
        "print(\"‚¨áÔ∏è Downloading dataset from Kaggle...\")\n",
        "dataset_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "# Extract ZIP if needed\n",
        "zip_path = Path(dataset_path) / \"chest-xray-pneumonia.zip\"\n",
        "if zip_path.exists():\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_path)\n",
        "data_dir = Path(dataset_path) / \"chest_xray\"\n",
        "\n",
        "# ========== 2. Data Preprocessing ==========\n",
        "print(\"üîÑ Preprocessing dataset...\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=data_dir / \"val\", transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir / \"test\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# ========== 3. SE Block ==========\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# ========== 4. Hybrid DenseNet + SE ==========\n",
        "class HybridDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(HybridDenseNet, self).__init__()\n",
        "        base_model = models.densenet121(pretrained=True)\n",
        "        self.features = base_model.features\n",
        "        self.se = SEBlock(1024)\n",
        "        self.classifier = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.se(x)\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1)).view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ========== 5. Training Setup ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = HybridDenseNet(num_classes=2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# ========== 6. Training Loop ==========\n",
        "print(\"üöÄ Training started...\")\n",
        "for epoch in range(1, 6):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch}: Loss = {running_loss:.4f}, Accuracy = {acc:.2f}%\")\n",
        "\n",
        "# ========== 7. Evaluation ==========\n",
        "print(\"üß™ Evaluating on test set...\")\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "\n",
        "print(\"üìâ Confusion Matrix:\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# ========== 1. Download Dataset ==========\n",
        "print(\"‚¨áÔ∏è Downloading dataset from Kaggle...\")\n",
        "dataset_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "# Extract ZIP if needed\n",
        "zip_path = Path(dataset_path) / \"chest-xray-pneumonia.zip\"\n",
        "if zip_path.exists():\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_path)\n",
        "data_dir = Path(dataset_path) / \"chest_xray\"\n",
        "\n",
        "# ========== 2. Data Preprocessing & Augmentation ==========\n",
        "print(\"üîÑ Preprocessing dataset with enhanced augmentation...\")\n",
        "\n",
        "# More aggressive data augmentation to improve generalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Larger size for initial resize\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random crop\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),  # Slight rotation\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Small translations\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Vary brightness/contrast\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Less augmentation for validation and test sets\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets with appropriate transforms\n",
        "train_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=data_dir / \"val\", transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir / \"test\", transform=eval_transform)\n",
        "\n",
        "# ========== 3. Handle Class Imbalance ==========\n",
        "# Calculate class weights based on dataset distribution\n",
        "train_targets = [label for _, label in train_dataset.samples]\n",
        "class_counts = np.bincount(train_targets)\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "sample_weights = class_weights[train_targets]\n",
        "\n",
        "# Create weighted sampler to balance classes\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# Create data loaders with appropriate batch sizes\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "print(f\"Class distribution in training set: {class_counts}\")\n",
        "\n",
        "# ========== 4. Improved SE Block ==========\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=8):  # Lower reduction ratio for better feature reweighting\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),  # Add dropout to prevent overfitting\n",
        "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# ========== 5. Enhanced DenseNet + SE ==========\n",
        "class EnhancedDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedDenseNet, self).__init__()\n",
        "        base_model = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "        # Freeze early layers to prevent overfitting\n",
        "        for param in list(base_model.features.parameters())[:30]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.features = base_model.features\n",
        "\n",
        "        # Add multiple SE blocks at different levels\n",
        "        self.se1 = SEBlock(1024)\n",
        "\n",
        "        # Additional layers for better feature extraction\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.3)  # Add dropout to reduce overfitting\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.se1(x)\n",
        "        x = self.global_pool(x).view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ========== 6. Training Setup with Improved Techniques ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "\n",
        "# Use weighted loss to further address class imbalance\n",
        "class_weights_tensor = class_weights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Use AdamW optimizer with weight decay to reduce overfitting\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "# Add learning rate scheduler to reduce LR when validation performance plateaus\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "best_model_path = 'best_pneumonia_model.pth'\n",
        "\n",
        "# ========== 7. Training Loop with Validation ==========\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return val_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "print(\"üöÄ Training started with improved techniques...\")\n",
        "num_epochs = 15  # Increased number of epochs\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs, val_accs = [], []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {epoch}/{num_epochs}:\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"  Model saved to {best_model_path}\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "            break\n",
        "\n",
        "# Load best model for evaluation\n",
        "print(f\"Loading best model from {best_model_path}\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# ========== 8. Evaluation with Detailed Metrics ==========\n",
        "print(\"üß™ Evaluating on test set...\")\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "all_probs = []  # For ROC curve\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_probs.extend(probabilities[:, 1].cpu().numpy())  # Probability of pneumonia class\n",
        "\n",
        "# Calculate and display metrics\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "report = classification_report(all_labels, all_preds, target_names=train_dataset.classes, output_dict=True)\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"üìâ Confusion Matrix:\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot training and validation curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.title('Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy')\n",
        "plt.plot(val_accs, label='Val Accuracy')\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.close()\n",
        "\n",
        "# Calculate per-class metrics\n",
        "normal_precision = report['NORMAL']['precision']\n",
        "normal_recall = report['NORMAL']['recall']\n",
        "pneumonia_precision = report['PNEUMONIA']['precision']\n",
        "pneumonia_recall = report['PNEUMONIA']['recall']\n",
        "overall_accuracy = report['accuracy']\n",
        "\n",
        "print(\"\\nüîç Summary of Improvements:\")\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "print(f\"NORMAL - Precision: {normal_precision:.4f}, Recall: {normal_recall:.4f}\")\n",
        "print(f\"PNEUMONIA - Precision: {pneumonia_precision:.4f}, Recall: {pneumonia_recall:.4f}\")\n",
        "\n",
        "# If there are still class imbalance issues, suggest further improvements\n",
        "if abs(normal_recall - pneumonia_recall) > 0.1:\n",
        "    print(\"\\n‚ö†Ô∏è There may still be class imbalance issues. Consider:\")\n",
        "    print(\"  - Further tuning of class weights\")\n",
        "    print(\"  - Exploring different model architectures\")\n",
        "    print(\"  - Additional data augmentation for the underperforming class\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m3Z3WAQJyX4",
        "outputId": "16837403-db40-4235-c27b-1a29a547d613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading dataset from Kaggle...\n",
            "üîÑ Preprocessing dataset with enhanced augmentation...\n",
            "Dataset sizes - Train: 5216, Val: 16, Test: 624\n",
            "Class distribution in training set: [1341 3875]\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 106MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training started with improved techniques...\n",
            "Epoch 1/15:\n",
            "  Train - Loss: 0.1683, Accuracy: 92.58%\n",
            "  Val   - Loss: 0.1962, Accuracy: 93.75%\n",
            "  Model saved to best_pneumonia_model.pth\n",
            "Epoch 2/15:\n",
            "  Train - Loss: 0.1066, Accuracy: 95.82%\n",
            "  Val   - Loss: 0.1115, Accuracy: 100.00%\n",
            "  Model saved to best_pneumonia_model.pth\n",
            "Epoch 3/15:\n",
            "  Train - Loss: 0.0802, Accuracy: 96.82%\n",
            "  Val   - Loss: 0.0712, Accuracy: 93.75%\n",
            "  Model saved to best_pneumonia_model.pth\n",
            "Epoch 4/15:\n",
            "  Train - Loss: 0.0649, Accuracy: 97.68%\n",
            "  Val   - Loss: 0.3148, Accuracy: 87.50%\n",
            "Epoch 5/15:\n",
            "  Train - Loss: 0.0491, Accuracy: 98.16%\n",
            "  Val   - Loss: 0.0075, Accuracy: 100.00%\n",
            "  Model saved to best_pneumonia_model.pth\n",
            "Epoch 6/15:\n",
            "  Train - Loss: 0.0648, Accuracy: 97.57%\n",
            "  Val   - Loss: 0.0899, Accuracy: 93.75%\n",
            "Epoch 7/15:\n",
            "  Train - Loss: 0.0574, Accuracy: 97.70%\n",
            "  Val   - Loss: 0.2327, Accuracy: 87.50%\n",
            "Epoch 8/15:\n",
            "  Train - Loss: 0.0492, Accuracy: 98.14%\n",
            "  Val   - Loss: 0.0252, Accuracy: 100.00%\n",
            "Epoch 9/15:\n",
            "  Train - Loss: 0.0302, Accuracy: 98.98%\n",
            "  Val   - Loss: 0.0082, Accuracy: 100.00%\n",
            "Epoch 10/15:\n",
            "  Train - Loss: 0.0254, Accuracy: 98.98%\n",
            "  Val   - Loss: 0.0055, Accuracy: 100.00%\n",
            "  Model saved to best_pneumonia_model.pth\n",
            "Epoch 11/15:\n",
            "  Train - Loss: 0.0320, Accuracy: 99.00%\n",
            "  Val   - Loss: 0.1033, Accuracy: 100.00%\n",
            "Epoch 12/15:\n",
            "  Train - Loss: 0.0214, Accuracy: 99.18%\n",
            "  Val   - Loss: 0.0279, Accuracy: 100.00%\n",
            "Epoch 13/15:\n",
            "  Train - Loss: 0.0197, Accuracy: 99.25%\n",
            "  Val   - Loss: 0.0828, Accuracy: 93.75%\n",
            "Epoch 14/15:\n",
            "  Train - Loss: 0.0202, Accuracy: 99.19%\n",
            "  Val   - Loss: 0.1189, Accuracy: 93.75%\n",
            "Epoch 15/15:\n",
            "  Train - Loss: 0.0115, Accuracy: 99.64%\n",
            "  Val   - Loss: 0.0839, Accuracy: 93.75%\n",
            "Early stopping triggered after 15 epochs\n",
            "Loading best model from best_pneumonia_model.pth\n",
            "üß™ Evaluating on test set...\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.96      0.93      0.94       234\n",
            "   PNEUMONIA       0.96      0.98      0.97       390\n",
            "\n",
            "    accuracy                           0.96       624\n",
            "   macro avg       0.96      0.95      0.96       624\n",
            "weighted avg       0.96      0.96      0.96       624\n",
            "\n",
            "üìâ Confusion Matrix:\n",
            "\n",
            "üîç Summary of Improvements:\n",
            "Overall Accuracy: 0.9583\n",
            "NORMAL - Precision: 0.9602, Recall: 0.9274\n",
            "PNEUMONIA - Precision: 0.9573, Recall: 0.9769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.utils.prune as prune\n",
        "import cv2\n",
        "\n",
        "# ========== 1. Download Dataset ==========\n",
        "print(\"‚¨áÔ∏è Downloading dataset from Kaggle...\")\n",
        "dataset_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "zip_path = Path(dataset_path) / \"chest-xray-pneumonia.zip\"\n",
        "if zip_path.exists():\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_path)\n",
        "data_dir = Path(dataset_path) / \"chest_xray\"\n",
        "\n",
        "# Note: Using Kaggle dataset due to access limitations. For research, use RSNA or CheXpert [14][15].\n",
        "\n",
        "# ========== 2. Data Preprocessing & Augmentation ==========\n",
        "print(\"üîÑ Preprocessing dataset with enhanced augmentation...\")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=data_dir / \"val\", transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir / \"test\", transform=eval_transform)\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "full_dataset.transform = eval_transform\n",
        "\n",
        "# ========== 3. Handle Class Imbalance ==========\n",
        "train_targets = [label for _, label in train_dataset.samples]\n",
        "class_counts = np.bincount(train_targets)\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "sample_weights = class_weights[train_targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# ========== 4. Fixed SE Block ==========\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        # Make sure reduction ratio doesn't result in dimensions too small\n",
        "        self.reduction = max(1, in_channels // reduction)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, self.reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# ========== 5. Fixed Enhanced DenseNet with Multiple SE Blocks and Grad-CAM ==========\n",
        "class EnhancedDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedDenseNet, self).__init__()\n",
        "        # Load the pre-trained DenseNet121 model\n",
        "        self.densenet = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "        # Get the correct number of input features for the classifier\n",
        "        num_ftrs = self.densenet.classifier.in_features  # This should be 1024 for DenseNet121\n",
        "\n",
        "        # Replace the classifier\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Add SE blocks to appropriate layers\n",
        "        # DenseNet121 has these channel sizes at different blocks:\n",
        "        # - After denseblock1: 256 channels\n",
        "        # - After denseblock2: 512 channels\n",
        "        # - After denseblock3: 1024 channels\n",
        "        # - After denseblock4: 1024 channels\n",
        "        self.se_blocks = nn.ModuleDict({\n",
        "            'se1': SEBlock(256),   # After denseblock1\n",
        "            'se2': SEBlock(512),   # After denseblock2\n",
        "            'se3': SEBlock(1024)   # After denseblock3 and denseblock4\n",
        "        })\n",
        "\n",
        "        self.gradients = None\n",
        "\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x, grad_cam=False):\n",
        "        # Initial layers\n",
        "        x = self.densenet.features.conv0(x)\n",
        "        x = self.densenet.features.norm0(x)\n",
        "        x = self.densenet.features.relu0(x)\n",
        "        x = self.densenet.features.pool0(x)\n",
        "\n",
        "        # DenseBlock1\n",
        "        x = self.densenet.features.denseblock1(x)\n",
        "        x = self.se_blocks['se1'](x)  # Apply first SE block\n",
        "        x = self.densenet.features.transition1(x)\n",
        "\n",
        "        # DenseBlock2\n",
        "        x = self.densenet.features.denseblock2(x)\n",
        "        x = self.se_blocks['se2'](x)  # Apply second SE block\n",
        "        x = self.densenet.features.transition2(x)\n",
        "\n",
        "        # DenseBlock3\n",
        "        x = self.densenet.features.denseblock3(x)\n",
        "        x = self.se_blocks['se3'](x)  # Apply third SE block\n",
        "        x = self.densenet.features.transition3(x)\n",
        "\n",
        "        # DenseBlock4 (final features)\n",
        "        features = self.densenet.features.denseblock4(x)\n",
        "        features = self.se_blocks['se3'](features)  # Reuse third SE block\n",
        "        features = self.densenet.features.norm5(features)\n",
        "\n",
        "        if grad_cam:\n",
        "            features.register_hook(self.activations_hook)\n",
        "\n",
        "        # Global pooling\n",
        "        out = nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        # Classifier\n",
        "        out = self.densenet.classifier(out)\n",
        "\n",
        "        return out, features\n",
        "\n",
        "    def get_gradcam(self, x, class_idx):\n",
        "        self.eval()\n",
        "        output, features = self.forward(x, grad_cam=True)\n",
        "\n",
        "        if output.shape[1] <= class_idx:\n",
        "            # Handle case where class_idx is out of bounds\n",
        "            class_idx = 0\n",
        "\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, class_idx] = 1\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        gradients = self.gradients\n",
        "        pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
        "        activations = features.detach()\n",
        "\n",
        "        for i in range(activations.size(1)):\n",
        "            activations[:, i, :, :] *= pooled_gradients[:, i]\n",
        "\n",
        "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "        heatmap = torch.clamp(heatmap, min=0)\n",
        "\n",
        "        # Normalize the heatmap safely\n",
        "        if torch.max(heatmap) > 0:\n",
        "            heatmap /= torch.max(heatmap)\n",
        "\n",
        "        return heatmap.cpu().numpy()\n",
        "\n",
        "# ========== 6. Training Setup with Improved Techniques ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "\n",
        "# Apply pruning only to convolutional layers (exclude classifier)\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):  # Prune only Conv2d layers\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# Use weighted loss\n",
        "class_weights_tensor = class_weights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "best_model_path = 'best_pneumonia_model.pth'\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return val_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "# ========== 7. K-Fold Cross-Validation ==========\n",
        "print(\"üîç Testing model forward pass with a single batch...\")\n",
        "# Get a single batch for testing\n",
        "for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    print(f\"Input batch shape: {images.shape}\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = model(images)\n",
        "        print(f\"‚úÖ Model forward pass successful! Output shape: {outputs.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Forward pass error: {e}\")\n",
        "    break\n",
        "\n",
        "print(\"üöÄ Training with 5-fold cross-validation...\")\n",
        "num_epochs = 2\n",
        "k_folds = 2\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
        "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "    val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "    # Initialize fresh model for each fold\n",
        "    model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "\n",
        "    # Apply pruning\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    best_fold_val_loss = float('inf')\n",
        "    fold_patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs} (Fold {fold + 1}):\")\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_fold_val_loss:\n",
        "            best_fold_val_loss = val_loss\n",
        "            fold_patience_counter = 0\n",
        "            torch.save(model.state_dict(), f'best_model_fold_{fold + 1}.pth')\n",
        "\n",
        "            # Also update the overall best model if this is the best we've seen\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(model.state_dict(), best_model_path)\n",
        "        else:\n",
        "            fold_patience_counter += 1\n",
        "            if fold_patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "                break\n",
        "\n",
        "    fold_results.append({\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'best_val_loss': best_fold_val_loss\n",
        "    })\n",
        "\n",
        "# ========== 8. Evaluation and Grad-CAM Visualization ==========\n",
        "print(\"üß™ Evaluating best model on test set...\")\n",
        "best_fold = np.argmin([res['best_val_loss'] for res in fold_results])\n",
        "print(f\"Best fold was fold {best_fold + 1} with validation loss {fold_results[best_fold]['best_val_loss']:.4f}\")\n",
        "\n",
        "# Load the best model\n",
        "model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "model.load_state_dict(torch.load(f'best_model_fold_{best_fold + 1}.pth'))\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs, _ = model(images)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_probs.extend(probabilities[:, 1].cpu().numpy())\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "report = classification_report(all_labels, all_preds, target_names=train_dataset.classes, output_dict=True)\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "avg_train_losses = np.mean([res['train_losses'] for res in fold_results], axis=0)\n",
        "avg_val_losses = np.mean([res['val_losses'] for res in fold_results], axis=0)\n",
        "avg_train_accs = np.mean([res['train_accs'] for res in fold_results], axis=0)\n",
        "avg_val_accs = np.mean([res['val_accs'] for res in fold_results], axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(avg_train_losses, label='Train Loss')\n",
        "plt.plot(avg_val_losses, label='Val Loss')\n",
        "plt.title('Average Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(avg_train_accs, label='Train Accuracy')\n",
        "plt.plot(avg_val_accs, label='Val Accuracy')\n",
        "plt.title('Average Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.close()\n",
        "\n",
        "def visualize_gradcam(model, dataset, idx, class_names):\n",
        "    model.eval()\n",
        "    img, label = dataset[idx]\n",
        "    img_input = img.unsqueeze(0).to(device)\n",
        "    pred, _ = model(img_input)\n",
        "    class_idx = torch.argmax(pred, dim=1).item()\n",
        "\n",
        "    heatmap = model.get_gradcam(img_input, class_idx)\n",
        "\n",
        "    img_np = img.permute(1, 2, 0).numpy()\n",
        "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    heatmap = cv2.resize(heatmap, (224, 224))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    superimposed_img = heatmap * 0.4 + img_np * 255\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_np)\n",
        "    plt.title(f\"Original Image\\nLabel: {class_names[label]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(superimposed_img.astype(np.uint8))\n",
        "    plt.title(f\"Grad-CAM\\nPredicted: {class_names[class_idx]}\")\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'gradcam_{idx}.png')\n",
        "    plt.close()\n",
        "\n",
        "print(\"üñºÔ∏è Generating Grad-CAM visualizations...\")\n",
        "try:\n",
        "    for i in range(5):\n",
        "        visualize_gradcam(model, test_dataset, i, train_dataset.classes)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating Grad-CAM: {e}\")\n",
        "\n",
        "# Get performance metrics\n",
        "try:\n",
        "    normal_precision = report['NORMAL']['precision']\n",
        "    normal_recall = report['NORMAL']['recall']\n",
        "    pneumonia_precision = report['PNEUMONIA']['precision']\n",
        "    pneumonia_recall = report['PNEUMONIA']['recall']\n",
        "    overall_accuracy = report['accuracy']\n",
        "\n",
        "    print(\"\\nüîç Summary of Improvements:\")\n",
        "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "    print(f\"NORMAL - Precision: {normal_precision:.4f}, Recall: {normal_recall:.4f}\")\n",
        "    print(f\"PNEUMONIA - Precision: {pneumonia_precision:.4f}, Recall: {pneumonia_recall:.4f}\")\n",
        "except KeyError:\n",
        "    print(\"Could not compute all metrics. Check class names in classification report.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aGh8lKcePiH7",
        "outputId": "f20b743c-781e-4a7f-c8a8-d5b657220039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading dataset from Kaggle...\n",
            "üîÑ Preprocessing dataset with enhanced augmentation...\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30.8M/30.8M [00:00<00:00, 213MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Testing model forward pass with a single batch...\n",
            "Input batch shape: torch.Size([32, 3, 224, 224])\n",
            "‚úÖ Model forward pass successful! Output shape: torch.Size([32, 2])\n",
            "üöÄ Training with 5-fold cross-validation...\n",
            "\n",
            "Fold 1/2\n",
            "Epoch 1/2 (Fold 1):\n",
            "  Train - Loss: 0.1906, Accuracy: 90.72%\n",
            "  Val   - Loss: 0.0745, Accuracy: 97.24%\n",
            "Epoch 2/2 (Fold 1):\n",
            "  Train - Loss: 0.0383, Accuracy: 99.04%\n",
            "  Val   - Loss: 0.0648, Accuracy: 98.16%\n",
            "\n",
            "Fold 2/2\n",
            "Epoch 1/2 (Fold 2):\n",
            "  Train - Loss: 0.1647, Accuracy: 92.94%\n",
            "  Val   - Loss: 0.0952, Accuracy: 96.36%\n",
            "Epoch 2/2 (Fold 2):\n",
            "  Train - Loss: 0.0372, Accuracy: 98.77%\n",
            "  Val   - Loss: 0.0699, Accuracy: 97.24%\n",
            "üß™ Evaluating best model on test set...\n",
            "Best fold was fold 1 with validation loss 0.0648\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for EnhancedDenseNet:\n\tMissing key(s) in state_dict: \"densenet.features.conv0.weight\", \"densenet.features.denseblock1.denselayer1.conv1.weight\", \"densenet.features.denseblock1.denselayer1.conv2.weight\", \"densenet.features.denseblock1.denselayer2.conv1.weight\", \"densenet.features.denseblock1.denselayer2.conv2.weight\", \"densenet.features.denseblock1.denselayer3.conv1.weight\", \"densenet.features.denseblock1.denselayer3.conv2.weight\", \"densenet.features.denseblock1.denselayer4.conv1.weight\", \"densenet.features.denseblock1.denselayer4.conv2.weight\", \"densenet.features.denseblock1.denselayer5.conv1.weight\", \"densenet.features.denseblock1.denselayer5.conv2.weight\", \"densenet.features.denseblock1.denselayer6.conv1.weight\", \"densenet.features.denseblock1.denselayer6.conv2.weight\", \"densenet.features.transition1.conv.weight\", \"densenet.features.denseblock2.denselayer1.conv1.weight\", \"densenet.features.denseblock2.denselayer1.conv2.weight\", \"densenet.features.denseblock2.denselayer2.conv1.weight\", \"densenet.features.denseblock2.denselayer2.conv2.weight\", \"densenet.features.denseblock2.denselayer3.conv1.weight\", \"densenet.features.denseblock2.denselayer3.conv2.weight\", \"densenet.features.denseblock2.denselayer4.conv1.weight\", \"densenet.features.denseblock2.denselayer4.conv2.weight\", \"densenet.features.denseblock2.denselayer5.conv1.weight\", \"densenet.features.denseblock2.denselayer5.conv2.weight\", \"densenet.features.denseblock2.denselayer6.conv1.weight\", \"densenet.features.denseblock2.denselayer6.conv2.weight\", \"densenet.features.denseblock2.denselayer7.conv1.weight\", \"densenet.features.denseblock2.denselayer7.conv2.weight\", \"densenet.features.denseblock2.denselayer8.conv1.weight\", \"densenet.features.denseblock2.denselayer8.conv2.weight\", \"densenet.features.denseblock2.denselayer9.conv1.weight\", \"densenet.features.denseblock2.denselayer9.conv2.weight\", \"densenet.features.denseblock2.denselayer10.conv1.weight\", \"densenet.features.denseblock2.denselayer10.conv2.weight\", \"densenet.features.denseblock2.denselayer11.conv1.weight\", \"densenet.features.denseblock2.denselayer11.conv2.weight\", \"densenet.features.denseblock2.denselayer12.conv1.weight\", \"densenet.features.denseblock2.denselayer12.conv2.weight\", \"densenet.features.transition2.conv.weight\", \"densenet.features.denseblock3.denselayer1.conv1.weight\", \"densenet.features.denseblock3.denselayer1.conv2.weight\", \"densenet.features.denseblock3.denselayer2.conv1.weight\", \"densenet.features.denseblock3.denselayer2.conv2.weight\", \"densenet.features.denseblock3.denselayer3.conv1.weight\", \"densenet.features.denseblock3.denselayer3.conv2.weight\", \"densenet.features.denseblock3.denselayer4.conv1.weight\", \"densenet.features.denseblock3.denselayer4.conv2.weight\", \"densenet.features.denseblock3.denselayer5.conv1.weight\", \"densenet.features.denseblock3.denselayer5.conv2.weight\", \"densenet.features.denseblock3.denselayer6.conv1.weight\", \"densenet.features.denseblock3.denselayer6.conv2.weight\", \"densenet.features.denseblock3.denselayer7.conv1.weight\", \"densenet.features.denseblock3.denselayer7.conv2.weight\", \"densenet.features.denseblock3.denselayer8.conv1.weight\", \"densenet.features.denseblock3.denselayer8.conv2.weight\", \"densenet.features.denseblock3.denselayer9.conv1.weight\", \"densenet.features.denseblock3.denselayer9.conv2.weight\", \"densenet.features.denseblock3.denselayer10.conv1.weight\", \"densenet.features.denseblock3.denselayer10.conv2.weight\", \"densenet.features.denseblock3.denselayer11.conv1.weight\", \"densenet.features.denseblock3.denselayer11.conv2.weight\", \"densenet.features.denseblock3.denselayer12.conv1.weight\", \"densenet.features.denseblock3.denselayer12.conv2.weight\", \"densenet.features.denseblock3.denselayer13.conv1.weight\", \"densenet.features.denseblock3.denselayer13.conv2.weight\", \"densenet.features.denseblock3.denselayer14.conv1.weight\", \"densenet.features.denseblock3.denselayer14.conv2.weight\", \"densenet.features.denseblock3.denselayer15.conv1.weight\", \"densenet.features.denseblock3.denselayer15.conv2.weight\", \"densenet.features.denseblock3.denselayer16.conv1.weight\", \"densenet.features.denseblock3.denselayer16.conv2.weight\", \"densenet.features.denseblock3.denselayer17.conv1.weight\", \"densenet.features.denseblock3.denselayer17.conv2.weight\", \"densenet.features.denseblock3.denselayer18.conv1.weight\", \"densenet.features.denseblock3.denselayer18.conv2.weight\", \"densenet.features.denseblock3.denselayer19.conv1.weight\", \"densenet.features.denseblock3.denselayer19.conv2.weight\", \"densenet.features.denseblock3.denselayer20.conv1.weight\", \"densenet.features.denseblock3.denselayer20.conv2.weight\", \"densenet.features.denseblock3.denselayer21.conv1.weight\", \"densenet.features.denseblock3.denselayer21.conv2.weight\", \"densenet.features.denseblock3.denselayer22.conv1.weight\", \"densenet.features.denseblock3.denselayer22.conv2.weight\", \"densenet.features.denseblock3.denselayer23.conv1.weight\", \"densenet.features.denseblock3.denselayer23.conv2.weight\", \"densenet.features.denseblock3.denselayer24.conv1.weight\", \"densenet.features.denseblock3.denselayer24.conv2.weight\", \"densenet.features.transition3.conv.weight\", \"densenet.features.denseblock4.denselayer1.conv1.weight\", \"densenet.features.denseblock4.denselayer1.conv2.weight\", \"densenet.features.denseblock4.denselayer2.conv1.weight\", \"densenet.features.denseblock4.denselayer2.conv2.weight\", \"densenet.features.denseblock4.denselayer3.conv1.weight\", \"densenet.features.denseblock4.denselayer3.conv2.weight\", \"densenet.features.denseblock4.denselayer4.conv1.weight\", \"densenet.features.denseblock4.denselayer4.conv2.weight\", \"densenet.features.denseblock4.denselayer5.conv1.weight\", \"densenet.features.denseblock4.denselayer5.conv2.weight\", \"densenet.features.denseblock4.denselayer6.conv1.weight\", \"densenet.features.denseblock4.denselayer6.conv2.weight\", \"densenet.features.denseblock4.denselayer7.conv1.weight\", \"densenet.features.denseblock4.denselayer7.conv2.weight\", \"densenet.features.denseblock4.denselayer8.conv1.weight\", \"densenet.features.denseblock4.denselayer8.conv2.weight\", \"densenet.features.denseblock4.denselayer9.conv1.weight\", \"densenet.features.denseblock4.denselayer9.conv2.weight\", \"densenet.features.denseblock4.denselayer10.conv1.weight\", \"densenet.features.denseblock4.denselayer10.conv2.weight\", \"densenet.features.denseblock4.denselayer11.conv1.weight\", \"densenet.features.denseblock4.denselayer11.conv2.weight\", \"densenet.features.denseblock4.denselayer12.conv1.weight\", \"densenet.features.denseblock4.denselayer12.conv2.weight\", \"densenet.features.denseblock4.denselayer13.conv1.weight\", \"densenet.features.denseblock4.denselayer13.conv2.weight\", \"densenet.features.denseblock4.denselayer14.conv1.weight\", \"densenet.features.denseblock4.denselayer14.conv2.weight\", \"densenet.features.denseblock4.denselayer15.conv1.weight\", \"densenet.features.denseblock4.denselayer15.conv2.weight\", \"densenet.features.denseblock4.denselayer16.conv1.weight\", \"densenet.features.denseblock4.denselayer16.conv2.weight\". \n\tUnexpected key(s) in state_dict: \"densenet.features.conv0.weight_orig\", \"densenet.features.conv0.weight_mask\", \"densenet.features.denseblock1.denselayer1.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer1.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer1.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer1.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer2.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer2.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer2.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer2.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer3.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer3.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer3.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer3.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer4.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer4.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer4.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer4.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer5.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer5.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer5.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer5.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer6.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer6.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer6.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer6.conv2.weight_mask\", \"densenet.features.transition1.conv.weight_orig\", \"densenet.features.transition1.conv.weight_mask\", \"densenet.features.denseblock2.denselayer1.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer1.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer1.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer1.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer2.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer2.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer2.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer2.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer3.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer3.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer3.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer3.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer4.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer4.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer4.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer4.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer5.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer5.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer5.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer5.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer6.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer6.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer6.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer6.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer7.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer7.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer7.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer7.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer8.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer8.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer8.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer8.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer9.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer9.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer9.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer9.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer10.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer10.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer10.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer10.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer11.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer11.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer11.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer11.conv2.weight_mask\", \"densenet.features.denseblock2.denselayer12.conv1.weight_orig\", \"densenet.features.denseblock2.denselayer12.conv1.weight_mask\", \"densenet.features.denseblock2.denselayer12.conv2.weight_orig\", \"densenet.features.denseblock2.denselayer12.conv2.weight_mask\", \"densenet.features.transition2.conv.weight_orig\", \"densenet.features.transition2.conv.weight_mask\", \"densenet.features.denseblock3.denselayer1.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer1.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer1.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer1.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer2.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer2.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer2.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer2.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer3.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer3.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer3.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer3.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer4.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer4.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer4.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer4.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer5.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer5.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer5.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer5.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer6.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer6.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer6.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer6.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer7.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer7.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer7.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer7.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer8.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer8.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer8.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer8.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer9.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer9.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer9.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer9.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer10.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer10.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer10.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer10.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer11.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer11.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer11.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer11.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer12.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer12.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer12.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer12.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer13.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer13.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer13.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer13.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer14.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer14.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer14.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer14.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer15.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer15.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer15.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer15.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer16.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer16.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer16.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer16.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer17.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer17.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer17.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer17.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer18.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer18.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer18.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer18.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer19.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer19.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer19.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer19.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer20.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer20.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer20.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer20.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer21.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer21.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer21.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer21.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer22.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer22.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer22.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer22.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer23.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer23.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer23.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer23.conv2.weight_mask\", \"densenet.features.denseblock3.denselayer24.conv1.weight_orig\", \"densenet.features.denseblock3.denselayer24.conv1.weight_mask\", \"densenet.features.denseblock3.denselayer24.conv2.weight_orig\", \"densenet.features.denseblock3.denselayer24.conv2.weight_mask\", \"densenet.features.transition3.conv.weight_orig\", \"densenet.features.transition3.conv.weight_mask\", \"densenet.features.denseblock4.denselayer1.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer1.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer1.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer1.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer2.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer2.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer2.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer2.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer3.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer3.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer3.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer3.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer4.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer4.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer4.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer4.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer5.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer5.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer5.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer5.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer6.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer6.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer6.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer6.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer7.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer7.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer7.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer7.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer8.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer8.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer8.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer8.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer9.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer9.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer9.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer9.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer10.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer10.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer10.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer10.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer11.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer11.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer11.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer11.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer12.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer12.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer12.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer12.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer13.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer13.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer13.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer13.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer14.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer14.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer14.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer14.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer15.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer15.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer15.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer15.conv2.weight_mask\", \"densenet.features.denseblock4.denselayer16.conv1.weight_orig\", \"densenet.features.denseblock4.denselayer16.conv1.weight_mask\", \"densenet.features.denseblock4.denselayer16.conv2.weight_orig\", \"densenet.features.denseblock4.denselayer16.conv2.weight_mask\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0bd055cc3ef8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnhancedDenseNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'best_model_fold_{best_fold + 1}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EnhancedDenseNet:\n\tMissing key(s) in state_dict: \"densenet.features.conv0.weight\", \"densenet.features.denseblock1.denselayer1.conv1.weight\", \"densenet.features.denseblock1.denselayer1.conv2.weight\", \"densenet.features.denseblock1.denselayer2.conv1.weight\", \"densenet.features.denseblock1.denselayer2.conv2.weight\", \"densenet.features.denseblock1.denselayer3.conv1.weight\", \"densenet.features.denseblock1.denselayer3.conv2.weight\", \"densenet.features.denseblock1.denselayer4.conv1.weight\", \"densenet.features.denseblock1.denselayer4.conv2.weight\", \"densenet.features.denseblock1.denselayer5.conv1.weight\", \"densenet.features.denseblock1.denselayer5.conv2.weight\", \"densenet.features.denseblock1.denselayer6.conv1.weight\", \"densenet.features.denseblock1.denselayer6.conv2.weight\", \"densenet.features.transition1.conv.weight\", \"densenet.features.denseblock2.denselayer1.conv1.weight\", \"densenet.features.denseblock2.denselayer1.conv2.weight\", \"densenet.features.denseblock2.denselayer2.conv1.weight\", \"densenet.features.denseblock2.denselayer2.conv2.weight\", \"densenet.features.denseblock2.denselayer3.conv1.weight\", \"densenet.features.denseblock2.denselayer3.conv2.weight\", \"densenet.features.denseblock2.denselayer4.conv1.weight\", \"densenet.features.denseblock2.denselayer4.conv2.weight\", \"densenet.features.denseblock2.denselayer5.conv1.weight\", \"densenet.features.denseblock2.denselayer5.conv2.weight\", \"densenet.features.denseblock2.denselayer6.conv1.weight\", \"densenet.features.denseblock2.denselayer6.conv2.weig...\n\tUnexpected key(s) in state_dict: \"densenet.features.conv0.weight_orig\", \"densenet.features.conv0.weight_mask\", \"densenet.features.denseblock1.denselayer1.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer1.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer1.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer1.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer2.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer2.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer2.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer2.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer3.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer3.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer3.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer3.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer4.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer4.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer4.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer4.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer5.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer5.conv1.weight_mask\", \"densenet.features.denseblock1.denselayer5.conv2.weight_orig\", \"densenet.features.denseblock1.denselayer5.conv2.weight_mask\", \"densenet.features.denseblock1.denselayer6.conv1.weight_orig\", \"densenet.features.denseblock1.denselayer6.conv1.weight_mask\",..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.utils.prune as prune\n",
        "import cv2\n",
        "\n",
        "# ========== 1. Download Dataset ==========\n",
        "print(\"‚¨áÔ∏è Downloading dataset from Kaggle...\")\n",
        "dataset_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "zip_path = Path(dataset_path) / \"chest-xray-pneumonia.zip\"\n",
        "if zip_path.exists():\n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dataset_path)\n",
        "data_dir = Path(dataset_path) / \"chest_xray\"\n",
        "\n",
        "# Note: Using Kaggle dataset due to access limitations. For research, use RSNA or CheXpert [14][15].\n",
        "\n",
        "# ========== 2. Data Preprocessing & Augmentation ==========\n",
        "print(\"üîÑ Preprocessing dataset with enhanced augmentation...\")\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "val_dataset = datasets.ImageFolder(root=data_dir / \"val\", transform=eval_transform)\n",
        "test_dataset = datasets.ImageFolder(root=data_dir / \"test\", transform=eval_transform)\n",
        "\n",
        "full_dataset = datasets.ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
        "full_dataset.transform = eval_transform\n",
        "\n",
        "# ========== 3. Handle Class Imbalance ==========\n",
        "train_targets = [label for _, label in train_dataset.samples]\n",
        "class_counts = np.bincount(train_targets)\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "sample_weights = class_weights[train_targets]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# Define data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# ========== 4. Fixed SE Block ==========\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        # Make sure reduction ratio doesn't result in dimensions too small\n",
        "        self.reduction = max(1, in_channels // reduction)\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, self.reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.reduction, in_channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "# ========== 5. Fixed Enhanced DenseNet with Multiple SE Blocks and Grad-CAM ==========\n",
        "class EnhancedDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(EnhancedDenseNet, self).__init__()\n",
        "        # Load the pre-trained DenseNet121 model\n",
        "        self.densenet = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "        # Get the correct number of input features for the classifier\n",
        "        num_ftrs = self.densenet.classifier.in_features  # This should be 1024 for DenseNet121\n",
        "\n",
        "        # Replace the classifier\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "        # Add SE blocks to appropriate layers\n",
        "        # DenseNet121 has these channel sizes at different blocks:\n",
        "        # - After denseblock1: 256 channels\n",
        "        # - After denseblock2: 512 channels\n",
        "        # - After denseblock3: 1024 channels\n",
        "        # - After denseblock4: 1024 channels\n",
        "        self.se_blocks = nn.ModuleDict({\n",
        "            'se1': SEBlock(256),   # After denseblock1\n",
        "            'se2': SEBlock(512),   # After denseblock2\n",
        "            'se3': SEBlock(1024)   # After denseblock3 and denseblock4\n",
        "        })\n",
        "\n",
        "        self.gradients = None\n",
        "\n",
        "    def activations_hook(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x, grad_cam=False):\n",
        "        # Initial layers\n",
        "        x = self.densenet.features.conv0(x)\n",
        "        x = self.densenet.features.norm0(x)\n",
        "        x = self.densenet.features.relu0(x)\n",
        "        x = self.densenet.features.pool0(x)\n",
        "\n",
        "        # DenseBlock1\n",
        "        x = self.densenet.features.denseblock1(x)\n",
        "        x = self.se_blocks['se1'](x)  # Apply first SE block\n",
        "        x = self.densenet.features.transition1(x)\n",
        "\n",
        "        # DenseBlock2\n",
        "        x = self.densenet.features.denseblock2(x)\n",
        "        x = self.se_blocks['se2'](x)  # Apply second SE block\n",
        "        x = self.densenet.features.transition2(x)\n",
        "\n",
        "        # DenseBlock3\n",
        "        x = self.densenet.features.denseblock3(x)\n",
        "        x = self.se_blocks['se3'](x)  # Apply third SE block\n",
        "        x = self.densenet.features.transition3(x)\n",
        "\n",
        "        # DenseBlock4 (final features)\n",
        "        features = self.densenet.features.denseblock4(x)\n",
        "        features = self.se_blocks['se3'](features)  # Reuse third SE block\n",
        "        features = self.densenet.features.norm5(features)\n",
        "\n",
        "        if grad_cam:\n",
        "            features.register_hook(self.activations_hook)\n",
        "\n",
        "        # Global pooling\n",
        "        out = nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        # Classifier\n",
        "        out = self.densenet.classifier(out)\n",
        "\n",
        "        return out, features\n",
        "\n",
        "    def get_gradcam(self, x, class_idx):\n",
        "        self.eval()\n",
        "        output, features = self.forward(x, grad_cam=True)\n",
        "\n",
        "        if output.shape[1] <= class_idx:\n",
        "            # Handle case where class_idx is out of bounds\n",
        "            class_idx = 0\n",
        "\n",
        "        one_hot = torch.zeros_like(output)\n",
        "        one_hot[0, class_idx] = 1\n",
        "        output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        gradients = self.gradients\n",
        "        pooled_gradients = torch.mean(gradients, dim=[2, 3])\n",
        "        activations = features.detach()\n",
        "\n",
        "        for i in range(activations.size(1)):\n",
        "            activations[:, i, :, :] *= pooled_gradients[:, i]\n",
        "\n",
        "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "        heatmap = torch.clamp(heatmap, min=0)\n",
        "\n",
        "        # Normalize the heatmap safely\n",
        "        if torch.max(heatmap) > 0:\n",
        "            heatmap /= torch.max(heatmap)\n",
        "\n",
        "        return heatmap.cpu().numpy()\n",
        "\n",
        "# Helper function to save model with pruning compatibility\n",
        "def save_model_state(model, filepath):\n",
        "    \"\"\"Save model state while handling pruned parameters\"\"\"\n",
        "    # Temporarily remove pruning to get clean state dict\n",
        "    pruned_modules = []\n",
        "    for name, module in model.named_modules():\n",
        "        if hasattr(module, 'weight_orig'):  # This indicates pruning was applied\n",
        "            pruned_modules.append((name, module))\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    # Save the clean state dict\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "\n",
        "    # Reapply pruning\n",
        "    for name, module in pruned_modules:\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# Helper function to load model state with pruning compatibility\n",
        "def load_model_state(model, filepath):\n",
        "    \"\"\"Load model state and reapply pruning\"\"\"\n",
        "    # Load the state dict\n",
        "    state_dict = torch.load(filepath)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Apply pruning after loading\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):  # Prune only Conv2d layers\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# ========== 6. Training Setup with Improved Techniques ==========\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "\n",
        "# Apply pruning only to convolutional layers (exclude classifier)\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Conv2d):  # Prune only Conv2d layers\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "# Use weighted loss\n",
        "class_weights_tensor = class_weights.to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "best_model_path = 'best_pneumonia_model.pth'\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return val_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "# ========== 7. K-Fold Cross-Validation ==========\n",
        "print(\"üîç Testing model forward pass with a single batch...\")\n",
        "# Get a single batch for testing\n",
        "for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    print(f\"Input batch shape: {images.shape}\")\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = model(images)\n",
        "        print(f\"‚úÖ Model forward pass successful! Output shape: {outputs.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Forward pass error: {e}\")\n",
        "    break\n",
        "\n",
        "print(\"üöÄ Training with 5-fold cross-validation...\")\n",
        "num_epochs = 2\n",
        "k_folds = 2\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(full_dataset)):\n",
        "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(full_dataset, batch_size=32, sampler=train_subsampler)\n",
        "    val_loader = DataLoader(full_dataset, batch_size=32, sampler=val_subsampler)\n",
        "\n",
        "    # Initialize fresh model for each fold\n",
        "    model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "\n",
        "    # Apply pruning\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.l1_unstructured(module, name='weight', amount=0.3)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    best_fold_val_loss = float('inf')\n",
        "    fold_patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs} (Fold {fold + 1}):\")\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
        "        print(f\"  Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_fold_val_loss:\n",
        "            best_fold_val_loss = val_loss\n",
        "            fold_patience_counter = 0\n",
        "            # Use the helper function to save with pruning compatibility\n",
        "            save_model_state(model, f'best_model_fold_{fold + 1}.pth')\n",
        "\n",
        "            # Also update the overall best model if this is the best we've seen\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                save_model_state(model, best_model_path)\n",
        "        else:\n",
        "            fold_patience_counter += 1\n",
        "            if fold_patience_counter >= patience:\n",
        "                print(f\"Early stopping triggered after {epoch} epochs\")\n",
        "                break\n",
        "\n",
        "    fold_results.append({\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'best_val_loss': best_fold_val_loss\n",
        "    })\n",
        "\n",
        "# ========== 8. Evaluation and Grad-CAM Visualization ==========\n",
        "print(\"üß™ Evaluating best model on test set...\")\n",
        "best_fold = np.argmin([res['best_val_loss'] for res in fold_results])\n",
        "print(f\"Best fold was fold {best_fold + 1} with validation loss {fold_results[best_fold]['best_val_loss']:.4f}\")\n",
        "\n",
        "# Load the best model using the helper function\n",
        "model = EnhancedDenseNet(num_classes=2).to(device)\n",
        "load_model_state(model, f'best_model_fold_{best_fold + 1}.pth')\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs, _ = model(images)\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_probs.extend(probabilities[:, 1].cpu().numpy())\n",
        "\n",
        "print(\"\\nüìä Classification Report:\")\n",
        "report = classification_report(all_labels, all_preds, target_names=train_dataset.classes, output_dict=True)\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes,\n",
        "            yticklabels=train_dataset.classes, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "avg_train_losses = np.mean([res['train_losses'] for res in fold_results], axis=0)\n",
        "avg_val_losses = np.mean([res['val_losses'] for res in fold_results], axis=0)\n",
        "avg_train_accs = np.mean([res['train_accs'] for res in fold_results], axis=0)\n",
        "avg_val_accs = np.mean([res['val_accs'] for res in fold_results], axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(avg_train_losses, label='Train Loss')\n",
        "plt.plot(avg_val_losses, label='Val Loss')\n",
        "plt.title('Average Loss Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(avg_train_accs, label='Train Accuracy')\n",
        "plt.plot(avg_val_accs, label='Val Accuracy')\n",
        "plt.title('Average Accuracy Curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves.png')\n",
        "plt.close()\n",
        "\n",
        "def visualize_gradcam(model, dataset, idx, class_names):\n",
        "    model.eval()\n",
        "    img, label = dataset[idx]\n",
        "    img_input = img.unsqueeze(0).to(device)\n",
        "    pred, _ = model(img_input)\n",
        "    class_idx = torch.argmax(pred, dim=1).item()\n",
        "\n",
        "    heatmap = model.get_gradcam(img_input, class_idx)\n",
        "\n",
        "    img_np = img.permute(1, 2, 0).numpy()\n",
        "    img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    heatmap = cv2.resize(heatmap, (224, 224))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "    superimposed_img = heatmap * 0.4 + img_np * 255\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_np)\n",
        "    plt.title(f\"Original Image\\nLabel: {class_names[label]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(superimposed_img.astype(np.uint8))\n",
        "    plt.title(f\"Grad-CAM\\nPredicted: {class_names[class_idx]}\")\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'gradcam_{idx}.png')\n",
        "    plt.close()\n",
        "\n",
        "print(\"üñºÔ∏è Generating Grad-CAM visualizations...\")\n",
        "try:\n",
        "    for i in range(5):\n",
        "        visualize_gradcam(model, test_dataset, i, train_dataset.classes)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating Grad-CAM: {e}\")\n",
        "\n",
        "# Get performance metrics\n",
        "try:\n",
        "    normal_precision = report['NORMAL']['precision']\n",
        "    normal_recall = report['NORMAL']['recall']\n",
        "    pneumonia_precision = report['PNEUMONIA']['precision']\n",
        "    pneumonia_recall = report['PNEUMONIA']['recall']\n",
        "    overall_accuracy = report['accuracy']\n",
        "\n",
        "    print(\"\\nüîç Summary of Improvements:\")\n",
        "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
        "    print(f\"NORMAL - Precision: {normal_precision:.4f}, Recall: {normal_recall:.4f}\")\n",
        "    print(f\"PNEUMONIA - Precision: {pneumonia_precision:.4f}, Recall: {pneumonia_recall:.4f}\")\n",
        "except KeyError:\n",
        "    print(\"Could not compute all metrics. Check class names in classification report.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIOdCcYP8w9a",
        "outputId": "2e6a86e6-5d6a-4372-a6b8-fc38e01353b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Downloading dataset from Kaggle...\n",
            "üîÑ Preprocessing dataset with enhanced augmentation...\n",
            "Using device: cuda\n",
            "üîç Testing model forward pass with a single batch...\n",
            "Input batch shape: torch.Size([32, 3, 224, 224])\n",
            "‚úÖ Model forward pass successful! Output shape: torch.Size([32, 2])\n",
            "üöÄ Training with 5-fold cross-validation...\n",
            "\n",
            "Fold 1/2\n",
            "Epoch 1/2 (Fold 1):\n",
            "  Train - Loss: 0.1765, Accuracy: 93.33%\n",
            "  Val   - Loss: 0.0835, Accuracy: 97.74%\n",
            "Epoch 2/2 (Fold 1):\n",
            "  Train - Loss: 0.0451, Accuracy: 98.77%\n",
            "  Val   - Loss: 0.0698, Accuracy: 97.24%\n",
            "\n",
            "Fold 2/2\n",
            "Epoch 1/2 (Fold 2):\n",
            "  Train - Loss: 0.1479, Accuracy: 94.17%\n",
            "  Val   - Loss: 0.0834, Accuracy: 97.01%\n",
            "Epoch 2/2 (Fold 2):\n",
            "  Train - Loss: 0.0537, Accuracy: 98.62%\n",
            "  Val   - Loss: 0.0503, Accuracy: 98.35%\n",
            "üß™ Evaluating best model on test set...\n",
            "Best fold was fold 2 with validation loss 0.0503\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      NORMAL       0.99      0.58      0.73       234\n",
            "   PNEUMONIA       0.80      0.99      0.88       390\n",
            "\n",
            "    accuracy                           0.84       624\n",
            "   macro avg       0.89      0.79      0.81       624\n",
            "weighted avg       0.87      0.84      0.83       624\n",
            "\n",
            "üñºÔ∏è Generating Grad-CAM visualizations...\n",
            "\n",
            "üîç Summary of Improvements:\n",
            "Overall Accuracy: 0.8381\n",
            "NORMAL - Precision: 0.9854, Recall: 0.5769\n",
            "PNEUMONIA - Precision: 0.7967, Recall: 0.9949\n"
          ]
        }
      ]
    }
  ]
}